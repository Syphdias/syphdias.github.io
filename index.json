[{"content":"If I had to guess, why people use an Arch-based system, I would guess a big reason would be the AUR — even though it is not officially supported. It is a big part of the community and the appeal of Arch. This is a story how the AUR can break some things and the reason it is not officially supported.\nIt all started with a normal update. In my case I used the AUR helper paru to update all system packages and all AUR packages. Only obs-studio-tytan652 failed when trying to compile, but I rarely use OBS Studio (it will get fixed, eventually).\nBroken Electron-based Applications When I tried to launch my note taking app Obsidian, it never started. I dropped into a shell and tried again.\n❯ obsidian /usr/lib/electron25/electron: error while loading shared libraries: libdav1d.so.6: cannot open shared object file: No such file or directory I had seem behavior like this before after an update. When you update your kernel and you are still running the old kernel. If the running (old) kernel needs to load a new module it expects the old libraries but cannot find them as only the new ones are available.\nIt is generally also a bad idea to symlink libraries. If you would symlink the old library to the location of the old one, there is no guarantee that they are even remotely compatible.\nTip 1: Don\u0026rsquo;t break your system with symlinking shared libraries.\nFor the new-kernel-old-module problem a reboot usually suffices. However, electron apps would probably not use any kernel shared libraries. To make sure anyways, I did reboot.\nTip 2: Reboot after a upgrade if libraries cannot be found.\nAfter the reboot I was still not able to launch Obsidian or any other Electron-based app.\nElectron is a framework for building desktop applications. It uses the Chromium browser engine and Node.js to make it easy to build cross-platform applications — especially, if you are already familiar with building web apps.\nWhat is Actually Missing and Where Should it Come From? Since the issue seems to come from [electon] and not Obsidian itself I looked at electron directly which yielded the same error.\nWhen looking for library dependencies for the command electron, there are two red herrings. One is, that the package electron is only a meta package to link to the latest stable version of electron The second red herring is that /usr/bin/electron25 is a shell script to exec /usr/lib/electron25/electron. So to check for the shared library dependencies, we can follow the trail like this:\n❯ which electron /usr/bin/electron ❯ ls -l /usr/bin/electron lrwxrwxrwx 1 root root 10 Jun 16 08:50 /usr/bin/electron -\u0026gt; electron25 ❯ file /usr/bin/electron25 /usr/bin/electron25: Bourne-Again shell script, ASCII text executable ❯ cat /usr/bin/electron25 #!/usr/bin/bash […snip…] name=electron25 […snip…] exec /usr/lib/${name}/electron \u0026#34;${flags[@]}\u0026#34; \u0026#34;$@\u0026#34; ❯ file /usr/lib/electron25/electron /usr/lib/electron25/electron: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, for GNU/Linux 4.4.0, BuildID[sha1]=adabc98fbf2c1422ad6b6c4de371150f4fe605aa, stripped Tip 3: Find the binary that is actually run.\nTo find the libraries you can run ldd and filtered for the library name.\n❯ ldd /usr/lib/electron25/electron | grep libdav1d libdav1d.so.7 =\u0026gt; /usr/lib/libdav1d.so.7 (0x00007f41ea424000) ❯ ls -l /usr/lib/libdav1d.so.7 lrwxrwxrwx 1 root root 17 Oct 4 17:21 /usr/lib/libdav1d.so.7 -\u0026gt; libdav1d.so.7.0.0 ❯ pacman -F /usr/lib/libdav1d.so.7 usr/lib/libdav1d.so.7 is owned by extra/dav1d 1.3.0-1 Let\u0026rsquo;s walk through this. The binary wants libdav1d.so.7 and it can be found in /usr/lib/. The way this resolution from filename to path works is similar to how the PATH variable works. If you want to learn more about where libraries live, search for LS_LIBRARY_PATH and ld. For now, it is enough to know that the library is present at a know location, and points to another (existing) file.\nBut actually this library is not the one the error complains about. In the error above it complained about libdav1d.so.6 being absent.\nI reached out to the Arch Linux Mailing List which was very helpful. With the help of lddtree (from the package pax-utils) I could look recursively at the required libraries. Again, I filtered for the relevant library name but show 5 lines above the matches to see potential parent dependencies.\n❯ lddtree /usr/lib/electron25/electron | grep -B5 libdav1d libavcodec.so.60 =\u0026gt; /usr/lib/libavcodec.so.60 libswresample.so.4 =\u0026gt; /usr/lib/libswresample.so.4 libsoxr.so.0 =\u0026gt; /usr/lib/libsoxr.so.0 libgomp.so.1 =\u0026gt; /usr/lib/libgomp.so.1 libvpx.so.8 =\u0026gt; /usr/lib/libvpx.so.8 libdav1d.so.6 =\u0026gt; None -- libva-x11.so.2 =\u0026gt; /usr/lib/libva-x11.so.2 libX11-xcb.so.1 =\u0026gt; /usr/lib/libX11-xcb.so.1 libxcb-dri3.so.0 =\u0026gt; /usr/lib/libxcb-dri3.so.0 libvdpau.so.1 =\u0026gt; /usr/lib/libvdpau.so.1 libOpenCL.so.1 =\u0026gt; /usr/lib/libOpenCL.so.1 libdav1d.so.7 =\u0026gt; /usr/lib/libdav1d.so.7 This still shows libdav1d.so.7 at the root level of the dependencies but also libdav1d.so.6 as dependency of libavcodec.so.60. It cannot resolve the name to a library location so None gets displayed.\nSo where is this library from?\n❯ pacman -F libavcodec.so.60 extra/ffmpeg 2:6.0-12 usr/lib/libavcodec.so.60 ❯ pacman -Qi ffmpeg |grep -e Name -e Prov -e Requi Name : ffmpeg-obs Provides : ffmpeg=6.0.r12.ga6dc929 libavcodec.so=60-64 libavdevice.so=60-64 libavfilter.so=9-64 libavformat.so=60-64 libavutil.so=58-64 libpostproc.so=57-64 libswresample.so=4-64 libswscale.so=7-64 Required By : chromaprint chromium electron25 ferdium-bin firefox gst-libav krita obs-studio-tytan652 opencv peek qt5-webengine telegram-desktop thunderbird vlc-luajit Hm, do you notice something?\nTip 4: You can use ldd, lddtree (pax-utils) to find shared library dependencies that a binary need (is linked against).\nTip 5: You can use pacman -F to find the package for file and pacman -Qi to show information about a package, like actual name or which other packages require it.\nThe Culprit The library is provided by ffmpeg a widely used library for decoding of all kinds of media. But the installed version is not the regular version from the official arch (extra) repositories but ffmpeg-obs, a version from the AUR which is required by obs-studio-tytan652.\nSo why is it broken? Every time a library is updated, every program that uses it, needs to also be rebuilt to link against the latest version. If you are running only official packages from Arch Linux they take good care that if one library gets an update every program or library using it (recursively) will be updated as well to reflect the changed version. This is the reason why a partially upgraded system is unsupported. They cannot guarantee that the libraries match up.\nThe AUR on the other hand is independent from the official package repositories. If something changes in the official repos, the AUR package maintainer need to notice and react to it. If it is a binary package, they need to rebuild it themselves. If it is a package built from source it needs to be rebuilt on the users machine as well as soon as the new libraries are available (can also be done through a version increment, e.g. increase the suffix to -2).\nAt this point I had two options.\nRemove obs-studio-tytan652 and switch back to extra/ffmpeg (the officially) supported version Rebuilt ffmpeg-obs to link against the latest version of dav1d. When I realised what had happened there was already a new version of ffmpeg-obs. But I will keep this in mind for future updates.\nConclusion If this error had happened in an AUR package I would have known that I probably had to rebuild it to link to the new dependencies. In this case however, one package in the dependency chain was replaced by an unofficial one. Which made it less obvious to track down the issue.\nTip 6: Be weary what packages you replace with packages from the AUR.\n","permalink":"https://www.relg.uk/posts/they-say-dont-use-the-aur/","summary":"\u003cp\u003eIf I had to guess, why people use an Arch-based system, I would guess a big\nreason would be the AUR — even though it is not officially supported. It is a\nbig part of the community and the appeal of Arch. This is a story how the AUR\ncan break some things and the reason it is not officially supported.\u003c/p\u003e\n\u003cp\u003eIt all started with a normal update. In my case I used the AUR helper \u003ca href=\"https://github.com/Morganamilo/paru\"\u003e\u003ccode\u003eparu\u003c/code\u003e\u003c/a\u003e\nto update all system packages and all AUR packages. Only \u003ccode\u003eobs-studio-tytan652\u003c/code\u003e\nfailed when trying to compile, but I rarely use OBS Studio (it will get fixed,\neventually).\u003c/p\u003e","title":"They Say Don't Use the AUR"},{"content":"I recently was looking for a way to run a systemd service on the last business day of the month, but I could only find an answer for first business day of every month on Stack Overflow which was wrong. So I looked into it.\nSpoiler: This is not possible with one calendar expression.\nIf you remember only one thing from this blog post, remember systemd-analyse. There are quite a few useful subcommands, e.g. verify. You should check them out, if you do not know them yet with systemd-analyse -h.\nFor timers, we want systemd-analyse calendar.\nFirst Business Day of the Month To achieve this we can set OnCalendar twice in the timer unit, we do not need two timer units:\n[Timer] OnCalendar=Mon..Fri *-*-01 OnCalendar=Mon *-*-02..03 Mon..Fri *-*-01 will activate if the first day of the month is a business day Mon *-*-02..03 will activate if the second or third day of the month is a Monday. This happens if the first was a Sunday, or if the first was a Saturday and the second was a Sunday. There is no overlap. To verify this is true we can use (--iterations is optional but very useful):\nsystemd-analyze calendar \u0026#39;Mon..Fri *-*-01\u0026#39; \u0026#39;Mon *-*-02..03\u0026#39; --iterations 10 The output is a bit unwieldy since it treats both expressions separately and prints three versions of the iterations.\nsystemd-analyze calendar \u0026#39;Mon..Fri *-*-01\u0026#39; \u0026#39;Mon *-*-02..03\u0026#39; --iterations 10 \\ |grep Iteration |sort -k4 This works for a English locale but it will filter out the first iteration because it is listed as \u0026ldquo;Next elapse\u0026rdquo;. The get a good overview in this case it suffices.\nLast Business Day of the Month Last day is very similar to first day, but requires dates that were introduced in systemd v233. Again, we need two expressions:\n[Timer] OnCalendar=Mon..Fri *-*~01 OnCalendar=Fri *-*~02..03 Mon..Fri *-*~01 triggers if the last day of the month is a business day Fri *-*~02..03 triggers if the second to last or third to last day of the month was a Friday. This happens if the weekend is on the last day of the month. Verifying this works just the same:\nsystemd-analyze calendar \u0026#39;Mon..Fri *-*~01\u0026#39; \u0026#39;Fri *-*~02..03\u0026#39; --iterations 10 Optionally, filter and sort with the caveats from above:\nsystemd-analyze calendar \u0026#39;Mon..Fri *-*~01\u0026#39; \u0026#39;Fri *-*~02..03\u0026#39; --iterations 10 \\ |grep Iteration |sort -k4 ","permalink":"https://www.relg.uk/posts/first-and-last-business-day-of-the-month-with-systemd-timers/","summary":"\u003cp\u003eI recently was looking for a way to run a systemd service on the last business\nday of the month, but I could only find an answer for first business day of\nevery month on Stack Overflow which was wrong. So I looked into it.\u003c/p\u003e\n\u003cp\u003eSpoiler: This is not possible with one calendar expression.\u003c/p\u003e\n\u003cp\u003eIf you remember only one thing from this blog post, \u003cstrong\u003eremember\n\u003ccode\u003esystemd-analyse\u003c/code\u003e\u003c/strong\u003e. There are quite a few useful subcommands, e.g. \u003ccode\u003everify\u003c/code\u003e.\nYou should check them out, if you do not know them yet with \u003ccode\u003esystemd-analyse -h\u003c/code\u003e.\u003c/p\u003e","title":"First and Last Business Day of the Month With Systemd Timers"},{"content":"I recently was consulted on a Ceph Cluster running into nearfull and backfillfull for the first time. One Ceph OSD was utilized over 85% and another over 90%. The operators were unaware of the meaning and what to do about it, so took a look.\nLooking at ceph status and ceph df, I noticed something. Try to spot it yourself – I made it easier by removing some stuff around it:\n$ ceph status [...] health: HEALTH_WARN 1 pools have many more objects per pg than average 1 backfillfull osd(s) 1 nearfull osd(s) Low space hindering backfill (add storage if this doesn\u0026#39;t resolve itself): 1 pg backfill_toofull 1 pgs not deep-scrubbed in time 1 pgs not scrubbed in time 20 pool(s) backfillfull services: [...] osd: 96 osds: 96 up (since 4w), 96 in (since 12M); 1 remapped pgs [...] data: volumes: 4/4 healthy pools: 20 pools, 4769 pgs objects: 31.90M objects, 117 TiB usage: 351 TiB used, 522 TiB / 873 TiB avail pgs: 299136/95689743 objects misplaced (0.313%) 4763 active+clean 5 active+clean+scrubbing+deep 1 active+remapped+backfill_toofull [...] # ceph df --- RAW STORAGE --- CLASS SIZE AVAIL USED RAW USED %RAW USED hdd 873 TiB 522 TiB 351 TiB 351 TiB 40.19 TOTAL 873 TiB 522 TiB 351 TiB 351 TiB 40.19 --- POOLS --- POOL ID PGS STORED OBJECTS USED %USED MAX AVAIL device_health_metrics 1 4096 907 MiB 108 2.7 GiB 0 14 TiB rbd 4 145 97 TiB 25.39M 290 TiB 87.43 14 TiB [...] The raw usage was only at 40%. Why would one disk contain so much data? The balancer was in upmap mode and active. But even with no balancer, this kind of miss-balancing would be extreme and very unlikely.\nYou may have already spotted something odd in the Ceph Pool configuration. While device_health_metrics contained less than 1GiB, it had 4096 PGs. At the same time rbd contained 97TiB in just 145 PGs.\n145 is not just no power of two (which would usually produce a Ceph Warning), but also way to low for the about of data and Ceph OSD count.\nWhat Does This Mean for Storage Distribution? Estimating the size of one PG for pool rbd yields about 685GiB (97TiB/145). How many (average sized) PGs will lead to utilization of one disk over 85%?\nAbout 11.4 (85% * 9TiB / 685GiB)\nUnfortunately, not every PG is the same size. Looking at the sizes, multiple PG exceed 800GiB. Furthermore not every Ceph OSD receives the same amount of PGs. And as we will soon see the number of PGs was trying to get lower.\nCause and Distributing Data But what actually caused the bogus PG numbers? The answer is: The PG Autoscaler. For some reason only device_health_metrics set a target_size_ratio to 0.1. This lead to the effective ratio to be 1 for this pool. Apparently the autoscaling assumed this would mean all data would be stored in this pool. This also explained why the number of PGs was not a power of two. The autoscaler set target_pg_num to 32 to reduce the pool rbd even more. This was why there was not Ceph Warning. This also means that if there were no disks were running full right now, it certainly would have happened in the following days.\nBefore removing the ratio, I wanted to know what would happen. I disabled autoscaling (ceph osd pool set noautoscale) and removed the target ratio:\n# ceph osd pool set device_health_metrics target_size_ratio 0 # ceph osd pool autoscale-status POOL SIZE TARGET SIZE RATE RAW CAPACITY RATIO TARGET RATIO EFFECTIVE RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE BULK device_health_metrics 906.5M 3.0 873.1T 0.0000 1.0 4096 1 on False rbd 99048G 3.0 873.1T 0.3323 1.0 32 1024 on False [...] This was a lot better and we decided to re-enable autoscaling (ceph osd pool unset noautoscale) right away.\nAfter a few minutes the backfillfull was gone. Soon to be followed by the nearfull. After a few days of rebalancing both pools had the proper PG count.\nI am not sure why the ratio was set and why it was interpreted as it was. The docs suggest this would not be a problem and I could not reproduce the behavior in a more recent version of Ceph. So this was potentially fixed already.\n","permalink":"https://www.relg.uk/posts/killing-your-ceph-with-autoscaling/","summary":"\u003cp\u003eI recently was consulted on a Ceph Cluster running into nearfull and backfillfull\nfor the first time. One Ceph OSD was utilized over 85% and another over 90%. The\noperators were unaware of the meaning and what to do about it, so took a look.\u003c/p\u003e\n\u003cp\u003eLooking at \u003ccode\u003eceph status\u003c/code\u003e and \u003ccode\u003eceph df\u003c/code\u003e, I noticed something. Try to spot it\nyourself – I made it easier by removing some stuff around it:\u003c/p\u003e","title":"Killing your Ceph with Autoscaling"},{"content":"I recently revived my Synology D415+ NAS from silicon death and it looks like it works fine again. When I bought it, I wanted to be able to run any docker image. Which is why I opted for Atom instead of ARM. Which is also why I upgraded RAM to 8GB. The disks basically never spun down which made it quite noisy. Now, I just want it to be silent, if not in use.\nI initially thought I could replace the DiskStation Manager (DSM) with a proper Linux but from what I gathered, it is \u0026ldquo;not doable without deeper knowledge\u0026rdquo;.\nAdjusting Fans I only inserted the first SSD so far and installed DSM 7.0 on it. The only audible sound was the fans spinning and I found a guide to use a custom fan profile to turn them off completely for low loads.\nStep one: Set \u0026ldquo;Fan Speed Mode\u0026rdquo; to \u0026ldquo;Quiet mode\u0026rdquo; via the GUI in \u0026ldquo;Hardware \u0026amp; Power\u0026rdquo;. This can also be done by setting\nStep two: Turning off fan check to allow for 0 speed operations.\n# cat /usr/local/etc/rc.d/fan_check_disable.sh \u0026lt;\u0026lt;EOF #!/bin/sh echo 0 \u0026gt; /sys/module/avoton_synobios/parameters/check_fan EOF # chmod 755 /usr/local/etc/rc.d/fan_check_disable.sh Step three: Backup /usr/syno/etc.defaults/scemd.xml and usr/syno/etc/scemd.xml Step four: Adjust fan profile as described in the guide with some modifications in /usr/syno/etc.defaults/scemd.xml and usr/syno/etc/scemd.xml.\n--- /usr/syno/etc/scemd.xml_2022-04-23\t2021-10-18 15:26:05.000000000 +0200 +++ /usr/syno/etc/scemd.xml\t2022-04-23 01:12:07.740595553 +0200 @@ -14,17 +14,21 @@ \u0026lt;cpu_temperature fan_speed=\u0026#34;99%40hz\u0026#34; action=\u0026#34;SHUTDOWN\u0026#34;\u0026gt;95\u0026lt;/cpu_temperature\u0026gt; \u0026lt;/fan_config\u0026gt; \u0026lt;fan_config period=\u0026#34;20\u0026#34; threshold=\u0026#34;6\u0026#34; type=\u0026#34;DUAL_MODE_LOW\u0026#34; hibernation_speed=\u0026#34;UNKNOWN\u0026#34;\u0026gt; -\t\u0026lt;disk_temperature fan_speed=\u0026#34;21%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;0\u0026lt;/disk_temperature\u0026gt; -\t\u0026lt;disk_temperature fan_speed=\u0026#34;35%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;42\u0026lt;/disk_temperature\u0026gt; -\t\u0026lt;disk_temperature fan_speed=\u0026#34;50%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;46\u0026lt;/disk_temperature\u0026gt; -\t\u0026lt;disk_temperature fan_speed=\u0026#34;70%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;53\u0026lt;/disk_temperature\u0026gt; +\t\u0026lt;disk_temperature fan_speed=\u0026#34;01%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;0\u0026lt;/disk_temperature\u0026gt; +\t\u0026lt;disk_temperature fan_speed=\u0026#34;10%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;41\u0026lt;/disk_temperature\u0026gt; +\t\u0026lt;disk_temperature fan_speed=\u0026#34;20%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;46\u0026lt;/disk_temperature\u0026gt; +\t\u0026lt;disk_temperature fan_speed=\u0026#34;35%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;48\u0026lt;/disk_temperature\u0026gt; +\t\u0026lt;disk_temperature fan_speed=\u0026#34;50%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;50\u0026lt;/disk_temperature\u0026gt; +\t\u0026lt;disk_temperature fan_speed=\u0026#34;70%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;54\u0026lt;/disk_temperature\u0026gt; \u0026lt;disk_temperature fan_speed=\u0026#34;99%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;58\u0026lt;/disk_temperature\u0026gt; \u0026lt;disk_temperature fan_speed=\u0026#34;99%40hz\u0026#34; action=\u0026#34;SHUTDOWN\u0026#34;\u0026gt;61\u0026lt;/disk_temperature\u0026gt; -\t\u0026lt;cpu_temperature fan_speed=\u0026#34;21%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;0\u0026lt;/cpu_temperature\u0026gt; -\t\u0026lt;cpu_temperature fan_speed=\u0026#34;50%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;50\u0026lt;/cpu_temperature\u0026gt; -\t\u0026lt;cpu_temperature fan_speed=\u0026#34;99%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;85\u0026lt;/cpu_temperature\u0026gt; -\t\u0026lt;cpu_temperature fan_speed=\u0026#34;99%40hz\u0026#34; action=\u0026#34;SHUTDOWN\u0026#34;\u0026gt;95\u0026lt;/cpu_temperature\u0026gt; +\t\u0026lt;cpu_temperature fan_speed=\u0026#34;01%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;0\u0026lt;/cpu_temperature\u0026gt; +\t\u0026lt;cpu_temperature fan_speed=\u0026#34;10%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;57\u0026lt;/cpu_temperature\u0026gt; +\t\u0026lt;cpu_temperature fan_speed=\u0026#34;20%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;62\u0026lt;/cpu_temperature\u0026gt; +\t\u0026lt;cpu_temperature fan_speed=\u0026#34;50%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;65\u0026lt;/cpu_temperature\u0026gt; +\t\u0026lt;cpu_temperature fan_speed=\u0026#34;99%40hz\u0026#34; action=\u0026#34;NONE\u0026#34;\u0026gt;80\u0026lt;/cpu_temperature\u0026gt; +\t\u0026lt;cpu_temperature fan_speed=\u0026#34;99%40hz\u0026#34; action=\u0026#34;SHUTDOWN\u0026#34;\u0026gt;90\u0026lt;/cpu_temperature\u0026gt; \u0026lt;/fan_config\u0026gt; \u0026lt;fan_config hw_version=\u0026#34;Synology-DX5\u0026#34; period=\u0026#34;20\u0026#34; threshold=\u0026#34;6\u0026#34; type=\u0026#34;DUAL_MODE_HIGH_EBOX\u0026#34; hibernation_speed=\u0026#34;FULL\u0026#34;\u0026gt; Step five: Reboot NAS\nAdding Disks If you checked out the source for the fan profile, you might have noticed that I skipped something more obvious. The IronWolf drives I bought are really loud when spun up – louder than the stock fan configuration in fact. For quiet operation they need to spin down when idle.\nLooking at the md configuration I noticed something odd.\n# cat /proc/mdstat Personalities : [raid1] md2 : active raid1 sda3[0] 239376512 blocks super 1.2 [1/1] [U] md1 : active raid1 sda2[0] 2097088 blocks [4/1] [U___] md0 : active raid1 sda1[0] 2490176 blocks [4/1] [U___] unused devices: \u0026lt;none\u0026gt; The first SSD (sda) I inserted was split into 3 partitions and turned into 3 arrays. md2 was the main bulk and what will is volume1. It is not mounted directly to /volume1 tough, but through /dev/mapper/cachedev_0. I assume this is to enable adding a caching device later (which makes no sense in this case) to be more flexible.\n# df -h /volume1/ Filesystem Size Used Avail Use% Mounted on /dev/mapper/cachedev_0 220G 92G 101G 48% /volume1 # ls -l /dev/mapper/cachedev_0 /dev/md2 brw------- 1 root root 253, 0 Apr 23 11:58 /dev/mapper/cachedev_0 brw------- 1 root root 9, 2 Apr 23 11:58 /dev/md2 # dmsetup ls --tree # ^^^^^^ notice major and minor device numbers match up cachedev_0 (253:0) └─ (9:2) md0(sda1) is used for / while I could not exactly figure out what md1(sda2) was used for – I read it was swap somewhere but I could not verify that, so don\u0026rsquo;t quote me on that.\n# df -h / Filesystem Size Used Avail Use% Mounted on /dev/md0 2.3G 1.2G 1.1G 52% / Notice [U___] on both md0 and md1 and what happens after I insert a second SSD and configure it as Storage Pool 2 (no volume) via the GUI.\n# cat /proc/mdstat Personalities : [raid1] md3 : active raid1 sdb3[0] 120212800 blocks super 1.2 [1/1] [U] md2 : active raid1 sda3[0] 239376512 blocks super 1.2 [1/1] [U] md1 : active raid1 sdb2[4] sda2[0] 2097088 blocks [4/1] [U___] [=======\u0026gt;.............] recovery = 37.0% (776576/2097088) finish=0.5min speed=36979K/sec md0 : active raid1 sdb1[4] sda1[0] 2490176 blocks [4/1] [U___] [=====\u0026gt;...............] recovery = 28.7% (716928/2490176) finish=0.9min speed=32587K/sec unused devices: \u0026lt;none\u0026gt; # cat /proc/mdstat Personalities : [raid1] md3 : active raid1 sdb3[0] 120212800 blocks super 1.2 [1/1] [U] md2 : active raid1 sda3[0] 239376512 blocks super 1.2 [1/1] [U] md1 : active raid1 sdb2[1] sda2[0] 2097088 blocks [4/2] [UU__] md0 : active raid1 sdb1[1] sda1[0] 2490176 blocks [4/2] [UU__] unused devices: \u0026lt;none\u0026gt; Initializing the new disk resulted in it being split into three partitions as well. It added the first two partitions to the RAID1 arrays md0 and md1 and left the third partition for usage. This is kind of clever. After the initial array sync – took a few seconds –, I could now remove the first SSD, if I wanted without losing my operating system. I would still lose everything I installed on the volume from this disk though. Indeed, this happens to every disk I insert and initialize a storage pool on.\nI inserted my 2 10T HDDs now, and indeed they undergo the same procedure. The array sync took about 1-2 minutes this time and I ended up with md4 and 2 more members for md0 and md1.\n# cat /proc/mdstat Personalities : [raid1] md4 : active raid1 sdd3[1] sdc3[0] 9761614848 blocks super 1.2 [2/2] [UU] [...] md1 : active raid1 sdd2[3] sdc2[2] sdb2[1] sda2[0] 2097088 blocks [4/4] [UUUU] md0 : active raid1 sdd1[3] sdc1[2] sdb1[1] sda1[0] 2490176 blocks [4/4] [UUUU] [...] This is great for simplicity and redundancy. I can yank any 3 disks and my system keeps running. But what if anything is read or written to the system disk? I expect this to hinder hibernation and spin down quite a bit.\nDebugging HDD Hibernation Part 1 I found the tool /usr/syno/sbin/syno_hibernation_debug to analyse hibernation fails (1 for \u0026ldquo;Cannot Enter Hibernation\u0026rdquo; or 2 for \u0026ldquo;Which Interrupt Hibernation\u0026rdquo;). The older one used in many forum posts was and no longer available (syno_hibernate_debug_tool --enable 1)\nI enabled the debug log by setting some options in /etc/synoinfo.conf\nenable_hibernation_debug=\u0026#34;yes\u0026#34; hibernation_debug_level=\u0026#34;1\u0026#34; I tailed /var/log/hibernationFull.log to get a feel what happens. I stopped the tail after I realised that reading a file from the disk I want to see idle might not work very well.\nAfter about 600 seconds the disks spun down only to spin up again 30 seconds later. I looked at the hibernation log to see what had happened.\n[Sat Apr 23 15:42:15 2022] ppid:1(systemd), pid:22779(syno_hibernatio), dirtied inode 20738 (hibernation.log) on md0 [Sat Apr 23 15:42:15 2022] ppid:1(systemd), pid:22779(syno_hibernatio), dirtied inode 20738 (hibernation.log) on md0 [Sat Apr 23 15:42:15 2022] ppid:1(systemd), pid:22779(syno_hibernatio), dirtied inode 20738 (hibernation.log) on md0 [Sat Apr 23 15:42:15 2022] ppid:1(systemd), pid:22779(syno_hibernatio), dirtied inode 30536 (hibernationFull.log) on md0 [Sat Apr 23 15:42:15 2022] ppid:1(systemd), pid:22779(syno_hibernatio), dirtied inode 30536 (hibernationFull.log) on md0 [Sat Apr 23 15:42:15 2022] ppid:1(systemd), pid:22779(syno_hibernatio), dirtied inode 30536 (hibernationFull.log) on md0 Huh, that\u0026rsquo;s the pid for the /usr/syno/sbin/syno_hibernation_debug process. Did it wake itself up?\nI looked deeper into to syno_hibernation_debug script to find out what was going on. The script mainly took care of a few things:\nReading configuration and running itself in the background if enable_hibernation_debug=\u0026quot;yes\u0026quot; was found and hibernation_debug_level was 1 or 2. If not, it killed the running debug process echo 1 \u0026gt; /proc/sys/vm/block_dump if enabled and echo 0 \u0026gt; /proc/sys/vm/block_dump if not it loops over /sys/block/sd{a..d}/device/syno_idle_time (in my case) to see if the idle time was below the configured standby time then it looks at dmesg | tail -500 it also writes to two log files (this should be find, it calls sync instantly afterwards and sleeps for 20s) This is when I gave up on the script. There really is nothing too special about it that I cannot do by hand without waking the disks up by myself. But I still have the feeling that internals might be at play that will always sync the array at some point in time. I don\u0026rsquo;t want the system to be on HDDs.\nI tried again, this time by hand.\necho 1 \u0026gt; /proc/sys/vm/block_dump while sleep 1s; do cat /sys/block/sd{a..d}/device/syno_idle_time; echo; done This time is was something else.\n[Sat Apr 23 16:41:17 2022] ppid:1(systemd), pid:14381(scemd), dirtied inode 30475 (disk_overview.xml) on md0 [Sat Apr 23 16:41:17 2022] ppid:1(systemd), pid:14381(scemd), dirtied inode 30475 (disk_overview.xml) on md0 I wonder, if I missed this the first time, but dmesg does not go back far enough and I am not willing to spend the time again. The other reason is that I will definitely do something about the HDDs in the array. Simply ssh-ing to the NAS wakes it up – reading authorized_keys or something? Probably more, logging, etc.?\nRemoving HDDs from Array The fact that the idle times if all four disks (2xSSD, 2xHDD) were almost always in sync to the same value, makes me sure that with this configuration it will be very hard to achieve total silence with spin down or even debug it properly.\nI removed the HDD partitions form arrays md0 and md1.\nmdadm /dev/md0 --fail /dev/sdc1 mdadm /dev/md0 --remove /dev/sdc1 mdadm /dev/md0 --fail /dev/sdd1 mdadm /dev/md0 --remove /dev/sdd1 mdadm /dev/md1 --fail /dev/sdc2 mdadm /dev/md1 --remove /dev/sdc2 mdadm /dev/md1 --fail /dev/sdd2 mdadm /dev/md1 --remove /dev/sdd2 Resulting in this (notice that I ):\n# cat /proc/mdstat Personalities : [raid1] md4 : active raid1 sdd3[1] sdc3[0] 9761614848 blocks super 1.2 [2/2] [UU] [...] md1 : active raid1 sdb2[1] sda2[0] 2097088 blocks [4/2] [UU__] md0 : active raid1 sdb1[1] sda1[0] 2490176 blocks [4/2] [UU__] [...] Synology did not like that. Important: Do not shrink the arrays. I did so at first (mdadm /dev/md0 --grow -n 2, same for md1) but when I later rebooted the machine, it went to the setup wizard instead. Fixing this was easy: I shut down the NAS, removed the HDDs and booted again. After it was done, I inserted the HDDs again, and assembled the array again (can be done via GUI).\nAdvice: Do not do this – unless you are sure of the consequences it might have. I expect this to break with OS upgrades and the like. Same for fan profiles.\nI looked at idle times again and was pleased to see that ssh-ing to the machine only reset idle times for the remaining SSDs in the system arrays.\nSadly that did not last long since the idle time rose above 10 minutes without the disks spinning down. Did Synology stop tracking the disk for some reason?\nTime to postpone for this day.\nDebugging HDD Hibernation Part 2 I noticed when the HDD did spin down, looking at /sys/block/sd{a..d}/device/syno_idle_time took longer. So this does not count as something to interrupt the idle time but it does wake up the disk. The syno_hibernation_debug script basically only works for checking why a disk does not enter hibernation but not for what wakes it up, because it will wake the disk up itself. All debugging efforts basically lead to spin-up – the German wiki page on hibernation even touches on something like this briefly.\nNew approach: I disabled all debugging measures (including hibernation log), closed all ssh connections, closed the webinterface and check for open connections to the machine (lsof -i@$hostname_of_NAS). Time for some series, a 10-15 minute timer and listening if the disks spin down at some point. I left echo 1 \u0026gt; /proc/sys/vm/block_dump to check for what woke up the drives if they did.\nWith Advanced Sleep After waiting about ten minutes the disks spun down and the yellow LEDs went off (Status and disks) while the blue power LED stayed solid. The fans did not spin up. (There was some network activity I saw on my router at a later date, mainly ARP and NTP).\nWhen I connected to the web GUI after over an hour the disks spun up and the LED went back on. Looking at dmesg and searching for the HDDs I saw some activity I assume to be a result of waking up from deep sleep and checking if anything changed with the disks.\n[Sun Apr 24 17:15:56 2022] sd 5:0:0:0: [sdd] Write cache: enabled, read cache: enabled, doesn\u0026#39;t support DPO or FUA [Sun Apr 24 17:16:01 2022] sd 4:0:0:0: [sdc] Write cache: disabled, read cache: enabled, doesn\u0026#39;t support DPO or FUA [Sun Apr 24 17:16:02 2022] sd 5:0:0:0: [sdd] Write cache: disabled, read cache: enabled, doesn\u0026#39;t support DPO or FUA [Sun Apr 24 17:16:26 2022] ppid:2(kthreadd), pid:30085(kworker/3:0), READ block 8 on sdd3 (8 sectors) [Sun Apr 24 17:16:26 2022] ppid:2(kthreadd), pid:30085(kworker/3:0), READ block 8 on sdc3 (8 sectors) [Sun Apr 24 17:16:26 2022] ppid:2(kthreadd), pid:6707(md4_raid1), WRITE block 8 on sdd3 (1 sectors) [Sun Apr 24 17:16:26 2022] ppid:2(kthreadd), pid:6707(md4_raid1), WRITE block 8 on sdc3 (1 sectors) [Sun Apr 24 17:16:26 2022] ppid:2(kthreadd), pid:6707(md4_raid1), WRITE block 8 on sdd3 (1 sectors) [Sun Apr 24 17:16:26 2022] ppid:2(kthreadd), pid:6707(md4_raid1), WRITE block 8 on sdc3 (1 sectors) I waited until the disk were silent again and the LEDs went off and connected via ssh – again, disk spin.\n[Sun Apr 24 17:29:55 2022] sd 4:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn\u0026#39;t support DPO or FUA Without Advanced Sleep My hope was, by not going into deep sleep to not wake the HDDs from their spin-down when connecting to the GUI or via ssh. After waiting 10 minutes the disks spun down but all LEDs stayed on. About every 10 minutes the fans spun up for 2-3 minutes but the disks stayed spun down. My guess would be that not going into deeps sleep the CPU and maybe the SSDs needed more power wich lead to higher temperature and thus a higher cooling need.\nWhen I connected via ssh the HDDs spun up. There was nothing for the disks in dmesg what had accessed the disks.\nI waited for the disks to go silent again and tried the same with connecting via the GUI. Unfortunately, the disks ramped up again.\n[Sun Apr 24 18:42:59 2022] ppid:2(kthreadd), pid:30021(kworker/2:1), READ block 8 on sdb3 (8 sectors) [Sun Apr 24 18:42:59 2022] ppid:2(kthreadd), pid:30021(kworker/2:1), READ block 8 on sda3 (8 sectors) Hunting Ghosts This is where I put down my pen for now and talk about what I have learned on this adventure.\nI guess my initial assumption that the disks didn\u0026rsquo;t properly spin down was false. Maybe I had installed something that kept disks awake back then or didn\u0026rsquo;t properly configure hibernate. Either way, do not assume something is (still) broken; verify before you invest time in debugging. For the mathematically inclined, for a complete induction it is essential to prove the base case before doing the induction step.\nMeasuring has its costs or there ain\u0026rsquo;t no such thing as a free lunch. When looking at metrics, surveying the data will change the data from what it would have been – reminds me of the uncertainty principle, somehow.\nWhat\u0026rsquo;s next I will keep my weird RAID configuration (for now), in case it did help. I am comfortable with dealing with potential fails that might occur. I moved the HDDs to the slots away from the mainboard to reduce heat radiating to them. I will keep the advanced/deep sleep configuration since there was no observable benefit from disabling deep sleep, if ssh-ing would wake up the HDDs anyways. I still do not understand this behavior and if anybody has an idea and how to fix it, please let me know.\nPS: I am aware that constantly spinning disks might not wear out as fast but this is a risk I am willing to take right now.\n","permalink":"https://www.relg.uk/posts/silent-synology/","summary":"\u003cp\u003eI recently revived my Synology D415+ NAS from \u003ca href=\"https://www.youtube.com/watch?v=PkZ0249t7SI\u0026t=339s\"\u003esilicon death\u003c/a\u003e and it looks like\nit works fine again. When I bought it, I wanted to be able to run any docker\nimage. Which is why I opted for Atom instead of ARM. Which is also why I upgraded\nRAM to 8GB. The disks basically never spun down which made it quite noisy.\nNow, I just want it to be silent, if not in use.\u003c/p\u003e","title":"Silent Synology"},{"content":"Recently re-inspired to start to blog, I decided to open – same as Jay Faulkner – with a meta post about why, what to expect, and how I (will) do it.\nWhy I Want to Blog If I start to learn a new topic, I feel like a total beginner. But the more I learn about something, the more I can draw from related topics to generate a more complete understanding of how it works. We all go through this. But at which point is it okay to talk about it as if you were knowledgable about the topic? I would say, do it earlier than you think, be honest about your state of understanding, and do not be afraid to be wrong (as long as it does not kill anyone). Writing is understanding.\nWhen you write – or speak – about a topic several things happen. You are forced to form complete sentences and express the idea as clear as you can. While you do, you might discover that you are still missing pieces or that your current understanding is wrong is some way. Another thing you might notice is, that you are unsure, if it is correct. Again, do not be afraid to be wrong but take the opportunity to learn more. Maybe there is another way to look at the concept that might be easier to explain. Writing about topic or concept can be helpful to someone reading it as well as writing about the learning experiences itself. My hope is to write about topics where I am on the way up from the valley of despair (Dunning–Kruger effect) or make the valley not as deep. I currently only write into my personal notes – mainly for documentation and findability. But it is too easy to just jot down bullet points and not actually understanding what you have written. I hope blogging deepens my understanding of things and also helps others understand the topic a little bit better.\nInspiration I read some other tech blogs like Florian Haas, Michael Stapelberg, and Kristian Köhntopp. I hugely respect all of them and enjoy their different content. I also enjoy the content liveoverflow produces on YouTube.\nWhat to Expect There is not set path for what you will find here in the future but I have some rough ideas what topics to cover. Subject to change but stuff I currently thing about:\ndebugging issues and learning experiences git, gnupg, gaming on Linux, i3, restic and systemd, scripts, vim, zsh Obsidian: documentation, note taking maybe opinions: remote work eventually: slides to talks How I Blog Most importantly – since this is something new to me – irregular and opportunistic. If I find something interesting enough, you will find it here.\nOn the technical side: The repo syphdias.github.io contains this page\u0026rsquo;s contents as markdown files and configuration files. GitHub Actions are being used to use hugo with the theme PaperMod to generate a static page on the branch gh-pages. It is then hosted by GitHub Pages.\nThere is a RSS/Atom feed you can use and I might enable comments if I am in the mood.\nIf you find mistakes in any of my posts, feel free to contribute a pull request.\n","permalink":"https://www.relg.uk/posts/starting-to-blog/","summary":"\u003cp\u003eRecently re-inspired to start to blog, I decided to open – same as\n\u003ca href=\"https://jay.jvf.cc/posts/about-itself/\"\u003eJay Faulkner\u003c/a\u003e – with a meta post about why, what to expect, and how I (will) do\nit.\u003c/p\u003e\n\u003ch2 id=\"why-i-want-to-blog\"\u003eWhy I Want to Blog\u003c/h2\u003e\n\u003cp\u003eIf I start to learn a new topic, I feel like a total beginner. But the more I\nlearn about something, the more I can draw from related topics to generate a\nmore complete understanding of how it works. We all go through this. But\nat which point is it okay to talk about it as if you were knowledgable about the\ntopic? I would say, do it earlier than you think, be honest about your state of\nunderstanding, and do not be afraid to be wrong (as long as it does not kill\nanyone). Writing is understanding.\u003c/p\u003e","title":"Starting to Blog"}]