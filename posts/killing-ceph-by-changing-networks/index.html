<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Killing Ceph by Changing Networks | relg.uk</title><meta name=keywords content><meta name=description content="Since a customer requested to change networks of Ceph first in 2023 and then
two more times, I felt like I had to put it in writing some time.
This use-case should be rare. But here the network design changed multiple
times — maybe this is this Agile I heard so much about. But in all
seriousness, network design should come first and not be subject to constant
change (unless you want to hire me)."><meta name=author content><link rel=canonical href=https://www.relg.uk/posts/killing-ceph-by-changing-networks/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://www.relg.uk/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.relg.uk/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.relg.uk/favicon-32x32.png><link rel=apple-touch-icon href=https://www.relg.uk/apple-touch-icon.png><link rel=mask-icon href=https://www.relg.uk/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://www.relg.uk/posts/killing-ceph-by-changing-networks/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://www.relg.uk/posts/killing-ceph-by-changing-networks/"><meta property="og:site_name" content="relg.uk"><meta property="og:title" content="Killing Ceph by Changing Networks"><meta property="og:description" content="Since a customer requested to change networks of Ceph first in 2023 and then two more times, I felt like I had to put it in writing some time.
This use-case should be rare. But here the network design changed multiple times — maybe this is this Agile I heard so much about. But in all seriousness, network design should come first and not be subject to constant change (unless you want to hire me)."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-05T20:11:02+01:00"><meta property="article:modified_time" content="2025-11-05T20:11:02+01:00"><meta name=fediverse:creator content="@syphdias@social.linux.pizza"><meta name=twitter:card content="summary"><meta name=twitter:title content="Killing Ceph by Changing Networks"><meta name=twitter:description content="Since a customer requested to change networks of Ceph first in 2023 and then
two more times, I felt like I had to put it in writing some time.
This use-case should be rare. But here the network design changed multiple
times — maybe this is this Agile I heard so much about. But in all
seriousness, network design should come first and not be subject to constant
change (unless you want to hire me)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.relg.uk/posts/"},{"@type":"ListItem","position":2,"name":"Killing Ceph by Changing Networks","item":"https://www.relg.uk/posts/killing-ceph-by-changing-networks/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Killing Ceph by Changing Networks","name":"Killing Ceph by Changing Networks","description":"Since a customer requested to change networks of Ceph first in 2023 and then two more times, I felt like I had to put it in writing some time.\nThis use-case should be rare. But here the network design changed multiple times — maybe this is this Agile I heard so much about. But in all seriousness, network design should come first and not be subject to constant change (unless you want to hire me).\n","keywords":[],"articleBody":"Since a customer requested to change networks of Ceph first in 2023 and then two more times, I felt like I had to put it in writing some time.\nThis use-case should be rare. But here the network design changed multiple times — maybe this is this Agile I heard so much about. But in all seriousness, network design should come first and not be subject to constant change (unless you want to hire me).\nWhen checking the Ceph documentation, I was pleasantly surprised! I made my fair share of negative experiences with Ceph Docs (web and cli help) but this part improved quite a bit for such a niche procedure.\nThere are currently three documented ways to change Network/IPs.\nScreenshot from the official Ceph documentation\n“The Right Way” While old documentation called it “The Right Way”, it is now called the “Preferred Method”.\nI wouldn’t even call it changing a Monitor’s IP since it basically adds a new Ceph Mon with a new IP, wait for the Quorum, remove the old Ceph Mon.\nThe documentation only suggest to do this in a manual fashion but in practice cephadm and rook should make this trivially easy…\n…as long as the new IPs are within the current network or at least reachable.\n“The Messy Way” While old documentation called it “The Messy Way”, it is now called “Advanced Method” — a bit boring, if you ask me since it can get messy, if you are unprepared.\nThe high-level step by step is basically:\nStop the Cluster Export the monmap Edit the monmap to remove/add Ceph Mons Inject monmap into Ceph Mons Start Mons, then rest of cluster This leaves out a few things like changing the public_network in the configuration database and how to deal with daemons and all the messiness that might come after.\n“The Third Way” Okay, I lied. There is no third way but a more detailed version of “The Messy Way” using cephadm.\nThe high-level steps are the same but this time done within cephadm shell including the resolution of some of the messiness and it still assumes you know how to get around your setup — if you don’t, please abort and ask for help!\nMy Take With My (Customer’s) Idiosyncrasies First off, start with a healthy cluster, if you can!\nSecondly, backup monmap, configuration, and keyrings where you can reach it (not /tmp of cephadm shell container or similar).\nI want to prevent data movement of any kind. It is probably not needed but I like the precaution.\nceph osd set noout ceph osd set norebalance ceph osd set nobackfill Stopping all Ceph services using Ansible and prevent them from starting when the hosts will be rebooted for the network changes to be properly applied.\nansible ceph_hosts -m systemd -a 'name=ceph.target state=stopped enabled=false' -b On one of the hosts I do more or less what “The Third Way” says. Change my monmap but with two important changes:\nUsing a volume for the cephadm shell only doing this once and then injecting the same monmap into all Ceph Mons root@ceph01:~# cephadm shell --name mon.ceph01 --mount /root:/roothome Inferring fsid f52ba149-31ea-4da9-9b11-17e745e20d35 Inferring config /var/lib/ceph/f52ba149-31ea-4da9-9b11-17e745e20d35/mon.ceph01/config Using recent ceph image quay.io/ceph/ceph@sha256:1b9158ce28975f95def6a0ad459fa19f1336506074267a4b47c1bd914a00fec0 root@ceph01:/# cd /roothome/ root@ceph01:/roothome# ceph-mon -i ceph01 --extract-monmap monmap-$(date +%F) 2025-11-05T08:56:17.050+0000 70bab6991834 -1 wrote monmap to monmap-2025-11-05 root@ceph01:/roothome# cp monmap-2025-11-05{,-new} Go to another terminal, back up the old monmap now!\nNow we remove the old IPs and add the new ones. I believe the up-to-date version of the add command more complicated than it needed to be and opted for the old syntax.\nroot@ceph01:/roothome# monmaptool --print monmap-2025-11-05-new monmaptool: monmap file monmap-2025-11-05-new epoch 10 fsid f52ba149-31ea-4da9-9b11-17e745e20d35 last_changed 2024-05-12T12:42:35.759936+0000 created 2022-01-13T13:05:50.128859+0000 min_mon_release 18 (reef) election_strategy: 1 0: [v2:10.10.10.1:3300/0,v1:10.10.10.1:6789/0] mon.ceph01 1: [v2:10.10.10.2:3300/0,v1:10.10.10.2:6789/0] mon.ceph02 2: [v2:10.10.10.3:3300/0,v1:10.10.10.3:6789/0] mon.ceph03 3: [v2:10.10.10.4:3300/0,v1:10.10.10.4:6789/0] mon.ceph04 4: [v2:10.10.10.5:3300/0,v1:10.10.10.5:6789/0] mon.ceph05 root@ceph01:/roothome# monmaptool --rm=ceph{01..05} monmap-2025-11-05-new monmaptool: monmap file monmap-2025-11-05-new monmaptool: removing ceph01 monmaptool: removing ceph02 monmaptool: removing ceph03 monmaptool: removing ceph04 monmaptool: removing ceph05 monmaptool: writing epoch 10 to monmap-2025-11-05-new (0 monitors) root@ceph01:/roothome# monmaptool --add ceph01 192.168.160.1 --add ceph02 192.168.160.2 --add ceph03 192.168.160.3 --add ceph04 192.168.160.4 --add ceph05 192.168.160.5 monmap-2025-11-05-new monmaptool: monmap file monmap-2025-11-05-new monmaptool: writing epoch 10 to monmap-2025-11-05-new (6 monitors) root@ceph01:/roothome# monmaptool --print monmap-2025-11-05-new monmaptool: monmap file monmap-2025-11-05-new epoch 10 fsid f52ba149-31ea-4da9-9b11-17e745e20d35 last_changed 2024-05-12T12:42:35.759936+0000 created 2022-01-13T13:05:50.128859+0000 min_mon_release 18 (reef) election_strategy: 1 0: [v2:192.168.160.1:3300/0,v1:192.168.160.1:6789/0] mon.ceph01 1: [v2:192.168.160.2:3300/0,v1:192.168.160.2:6789/0] mon.ceph02 2: [v2:192.168.160.3:3300/0,v1:192.168.160.3:6789/0] mon.ceph03 3: [v2:192.168.160.4:3300/0,v1:192.168.160.4:6789/0] mon.ceph04 4: [v2:192.168.160.5:3300/0,v1:192.168.160.5:6789/0] mon.ceph05 Ensure this is absolutely correct. When I did this I accidentally added another Ceph Mon that did not exist. Since there were still enough Ceph Mons to have a quorum, and this change needed to be done during a cluster downtime I was not worried about temporarily losing quorum if two more Ceph Mons failed or did not come up right away. I later removed it after the Ceph Mons were up again.\nSince I did not want to do these steps for every Ceph Mon host, I copied the new monmap to the relevant hosts, quick and dirty.\nfor i in {02..05}; do ssh ceph01 sudo cat /root/monmap-2025-11-05-new \\ | ssh ceph$i sudo tee /root/monmap-2025-11-05-new \u003e /dev/null done I ran the following commands to inject the monmap on each one of them, quick and dirty as well.\nfor i in {01..05}; do ssh ceph$i 'sudo cephadm shell --mount /root:/roothome --name mon.$HOSTNAME ceph-mon -i $HOSTNAME --inject-monmap /roothome/monmap-2025-11-05-new' done I updated /var/lib/ceph/{FSID}/mon.{MON}/config on every host. Either by replacing the IPs in the config or by running ceph config generate-minimal-conf in cephadm shell.\nSo far this was only preparations for the network change. In my case the network configuration is managed by an Ansible role, and I already made the necessary changes. This is the point of no return annoying rollback, if one would be needed for any reason.\nNote: For rolling back we would inject the old monmap for all Ceph Mons. After changing the network configuration on the hosts and the switches, this might be harder as we might not be able to easily connect to the hosts to deploy the old network configuration.\nansible-playbook site.yaml -t ceph_networking -D Since I am not in charge of the network hardware, this was the time to talk to the networking people, so they would switch over to the new configuration.\nansible ceph_hosts -m reboot -b After the reboots I checked the network using another Ansible script, that would check for bond failures, negotiated speed, VLAN tags, and finally pinging from every host to every other host on public and cluster network using jumbo frames. Depending on the cluster size this takes a while but is worth the effort in my experience to not run into weird issues due to missing MTU on some interfaces or other configuration errors (hardware or software).\nNote: To ping from everywhere to everywhere it builds a list of IPs filtered by CIDRs and loops over this list on every host.\nansible-playbook check-network.yaml Restart the cluster, if everything is okay.\nansible ceph_hosts -m systemd -a 'name=ceph.target state=started enabled=true' -b Setting public_network. I am not sure why it is set on mon and global level but there is no harm updating both.\nceph config set mon public_network 192.168.160.128/24 ceph config set global public_network 192.168.160.128/24 Fixing Ceph Mgr config.\nceph config generate-minimal-conf \u003e/var/lib/ceph/f52ba149-31ea-4da9-9b11-17e745e20d35/mgr.ceph01.yeinoh/config In my case I had to systemctl reset-failed 'ceph-*@mgr.ceph*' and start the service manually. Since the orchestrator does not know the new IPs yet, I needed to set them.\nceph orch host set-addr ceph01 192.168.160.1 # repeat for all hosts It will take a few minutes for the orchestrator to gather all information from every host again.\nThe documentation only mentions one last command to reconfigure the Ceph OSDs before it leaves you with the rest to fix for yourself. For example, to get a working standby Ceph Mgr and some other services I ran reconfig for all services on this cluster, including Ceph Mons to ensure there is no residue config referencing the old network.\nceph orch reconfig osd ceph orch reconfig mgr ceph orch reconfig mds ceph orch reconfig mon ceph orch reconfig rgw Note: I guess ceph orch redeploy $service would work as well but would take longer.\nNow we unset the flags to allow the cluster to function again normally.\nceph osd unset noout ceph osd unset norebalance ceph osd unset nobackfill Now for the supporting services, that need to be checked for functionality and I will try to include all relevant ones (including unused ones in this case):\nCeph Mgr should provide Ceph Dashboard with proper certificates and Ceph Exporter Observability tooling should be functional: Prometheus, Node-Exporter, Loki, Promtail, Grafana If you scrape from another external Prometheus you might need to change IPs to be scraped there If there are custom containers, e.g. S3 usage exporter, they should still be reachable Sync services functional: RGW Sync, RBD Mirror, and CephFS Mirror Change DNS RR(set)s for RGWs, etc. Lastly the RADOS clients (RGW, RBD, CephFS) need to be reconfigured to be able to communicate with the cluster again.\nAnd if there are firewalls in place, their rules should be updated beforehand; on hosts (iptables/nftables) and on centralized firewall appliances.\nConclusion While it is possible and fairly straight forward to move a Cluster to another or multiple other networks, I would advice to avoid the procedure. You might end up with no cluster at all, if you are not able to fix issues on the way. Even if you do not make mistakes during the process you might still encounter new hurdles with later versions or something might fail during the process.\nUnderstanding how to heal your cluster in an emergency is vital and you should have the ability to do so, if you attempt this.\nThis procedure really should not be a regular occurrence, which is why I did not automate it the first time and not the second time I had to do it.\n","wordCount":"1639","inLanguage":"en","datePublished":"2025-11-05T20:11:02+01:00","dateModified":"2025-11-05T20:11:02+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.relg.uk/posts/killing-ceph-by-changing-networks/"},"publisher":{"@type":"Organization","name":"relg.uk","logo":{"@type":"ImageObject","url":"https://www.relg.uk/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark"),document.addEventListener("DOMContentLoaded",function(){sessionStorage.getItem("nthLoad")?(document.getElementById("logo").classList.remove("logo-first-load"),document.getElementById("logo").classList.add("logo-nth-load")):sessionStorage.setItem("nthLoad","true")})</script><style>.logo{display:flex;font-family:MesloLGS NF,monospace;font-size:2em;cursor:pointer}.logo span{display:inline-block}.logo-first-load span{animation-delay:1s}@keyframes char1{0%{transform:translateX(0)}100%{transform:translateX(5ch)}}@keyframes char2{0%{transform:translateX(0)}100%{transform:translateX(3ch)}}@keyframes char3{0%{transform:translateX(0)}100%{transform:translateX(1ch)}}@keyframes char4{0%{transform:translateX(0)}100%{transform:translateX(-1ch)}}@keyframes char5{0%{transform:translateX(0);opacity:1}100%{transform:translateX(-2ch);opacity:0}}@keyframes char6{0%{transform:translateX(0)}100%{transform:translateX(-4ch)}}@keyframes char7{0%{transform:translateX(0)}100%{transform:translateX(-6ch)}}.logo-first-load span:nth-child(1){animation:char1 .5s ease 1s forwards}.logo-first-load span:nth-child(2){animation:char2 .5s ease 1s forwards}.logo-first-load span:nth-child(3){animation:char3 .5s ease 1s forwards}.logo-first-load span:nth-child(4){animation:char4 .5s ease 1s forwards}.logo-first-load span:nth-child(5){animation:char5 .5s ease 1s forwards}.logo-first-load span:nth-child(6){animation:char6 .5s ease 1s forwards}.logo-first-load span:nth-child(7){animation:char7 .5s ease 1s forwards}@keyframes char1r{0%{transform:translateX(5ch)}100%{transform:translateX(0)}}@keyframes char2r{0%{transform:translateX(3ch)}100%{transform:translateX(0)}}@keyframes char3r{0%{transform:translateX(1ch)}100%{transform:translateX(0)}}@keyframes char4r{0%{transform:translateX(-1ch)}100%{transform:translateX(0)}}@keyframes char5r{0%{transform:translateX(-2ch);opacity:0}100%{transform:translateX(0);opacity:1}}@keyframes char6r{0%{transform:translateX(-4ch)}100%{transform:translateX(0)}}@keyframes char7r{0%{transform:translateX(-6ch)}100%{transform:translateX(0)}}.logo-first-load:hover span{animation-delay:0s}.logo-first-load:hover span:nth-child(1){animation-name:char1r}.logo-first-load:hover span:nth-child(2){animation-name:char2r}.logo-first-load:hover span:nth-child(3){animation-name:char3r}.logo-first-load:hover span:nth-child(4){animation-name:char4r}.logo-first-load:hover span:nth-child(5){animation-name:char5r}.logo-first-load:hover span:nth-child(6){animation-name:char6r}.logo-first-load:hover span:nth-child(7){animation-name:char7r}.logo-nth-load span{transition:transform .5s ease,opacity .5s ease}.logo-nth-load:hover span:nth-child(1){transform:translateX(5ch)}.logo-nth-load:hover span:nth-child(2){transform:translateX(3ch)}.logo-nth-load:hover span:nth-child(3){transform:translateX(1ch)}.logo-nth-load:hover span:nth-child(4){transform:translateX(-1ch)}.logo-nth-load:hover span:nth-child(5){transform:translateX(-2ch);opacity:0}.logo-nth-load:hover span:nth-child(6){transform:translateX(-4ch)}.logo-nth-load:hover span:nth-child(7){transform:translateX(-6ch)}</style><header class=header><nav class=nav><div id=logo class="logo logo-first-load"><a href=https://www.relg.uk/ accesskey=h title="relg.uk (Alt + H)"><span>r</span><span>e</span><span>l</span><span>g</span><span>.</span><span>u</span><span>k</span></a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.relg.uk/search/ title="search (Alt + /)" accesskey=/><span><img src=/magnifying-glass-solid.svg alt=search style=height:1em;width:1em;display:inline-block;margin-right:.3em;vertical-align:middle>
search</span></a></li><li><a href=https://www.relg.uk/archives/ title=archive><span><img src=/box-archive-solid.svg alt=archive style=height:1em;width:1em;display:inline-block;margin-right:.3em;vertical-align:middle>
archive</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Killing Ceph by Changing Networks</h1><div class=post-meta><span title='2025-11-05 20:11:02 +0100 +0100'>2025-11-05</span>&nbsp;·&nbsp;8 min&nbsp;|&nbsp;<a href=https://github.com/syphdias/syphdias.github.io/edit/main/content/posts/killing-ceph-by-changing-networks.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>Since a customer requested to change networks of Ceph first in 2023 and then
two more times, I felt like I had to put it in writing some time.</p><p>This use-case should be rare. But here the network design changed multiple
times — maybe this is this <em>Agile</em> I heard so much about. But in all
seriousness, network design should come first and not be subject to constant
change (unless you want to hire me).</p><p>When checking the Ceph documentation, I was pleasantly surprised! I made my fair
share of negative experiences with Ceph Docs (web and cli help) but this part
improved quite a bit for such a niche procedure.</p><p>There are currently three documented ways to change Network/IPs.</p><p><img alt="Important: Existing monitors are not supposed to change their IP\naddresses." loading=lazy src=/assets/ceph-existing-mons.png>
<em>Screenshot from the official Ceph documentation</em></p><h2 id=the-right-way>&ldquo;The Right Way&rdquo;<a hidden class=anchor aria-hidden=true href=#the-right-way>#</a></h2><p>While old documentation called it <a href=https://docs.ceph.com/en/pacific/rados/operations/add-or-rm-mons/>&ldquo;The Right Way&rdquo;</a>, it is now called the
<a href=https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#changing-a-monitor-s-ip-address-preferred-method>&ldquo;Preferred Method&rdquo;</a>.</p><p>I wouldn&rsquo;t even call it <em>changing a Monitor&rsquo;s IP</em> since it basically adds a new
Ceph Mon with a new IP, wait for the Quorum, remove the old Ceph Mon.</p><p>The documentation only suggest to do this in a manual fashion but in practice
<a href=https://docs.ceph.com/en/latest/cephadm/>cephadm</a> and <a href=https://docs.ceph.com/en/latest/mgr/rook/>rook</a> should make this trivially easy…</p><p>…as long as the new IPs are within the current network or at least reachable.</p><h2 id=the-messy-way>&ldquo;The Messy Way&rdquo;<a hidden class=anchor aria-hidden=true href=#the-messy-way>#</a></h2><p>While old documentation called it <a href=https://docs.ceph.com/en/pacific/rados/operations/add-or-rm-mons/#changing-a-monitor-s-ip-address-the-messy-way>&ldquo;The Messy Way&rdquo;</a>, it is now called <a href=https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/#changing-a-monitor-s-ip-address-advanced-method>&ldquo;Advanced
Method&rdquo;</a> — a bit boring, if you ask me since it can get messy, if you are
unprepared.</p><p>The high-level step by step is basically:</p><ol><li>Stop the Cluster</li><li>Export the monmap</li><li>Edit the monmap to remove/add Ceph Mons</li><li>Inject monmap into Ceph Mons</li><li>Start Mons, then rest of cluster</li></ol><p>This leaves out a few things like changing the public_network in the
configuration database and how to deal with daemons and all the <em>messiness</em> that
might come after.</p><h2 id=the-third-way>&ldquo;The Third Way&rdquo;<a hidden class=anchor aria-hidden=true href=#the-third-way>#</a></h2><p>Okay, I lied. There is no third way but a more detailed version of <a href=/posts/killing-ceph-by-changing-networks/#the-messy-way>&ldquo;The Messy
Way&rdquo;</a> using cephadm.</p><p>The high-level steps are the same but this time done within <code>cephadm shell</code>
including the resolution of some of the <em>messiness</em> and it still assumes you
know how to get around your setup — if you don&rsquo;t, please abort and ask for help!</p><h2 id=my-take-with-my-customers-idiosyncrasies>My Take With My (Customer&rsquo;s) Idiosyncrasies<a hidden class=anchor aria-hidden=true href=#my-take-with-my-customers-idiosyncrasies>#</a></h2><p>First off, <strong>start with a healthy cluster</strong>, if you can!</p><p>Secondly, <strong>backup</strong> monmap, configuration, and keyrings where you can reach it
(not /tmp of cephadm shell container or similar).</p><p>I want to prevent data movement of any kind. It is probably not needed but I
like the precaution.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>ceph osd set noout
</span></span><span style=display:flex><span>ceph osd set norebalance
</span></span><span style=display:flex><span>ceph osd set nobackfill
</span></span></code></pre></div><p>Stopping all Ceph services using Ansible and prevent them from starting when the
hosts will be rebooted for the network changes to be properly applied.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>ansible ceph_hosts -m systemd -a <span style=color:#e6db74>&#39;name=ceph.target state=stopped enabled=false&#39;</span> -b
</span></span></code></pre></div><p>On one of the hosts I do more or less what <a href=/posts/killing-ceph-by-changing-networks/#the-third-way>&ldquo;The Third Way&rdquo;</a>
says. Change my monmap but with two important changes:</p><ul><li>Using a volume for the <code>cephadm shell</code></li><li>only doing this once and then injecting the same monmap into all Ceph Mons</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>root@ceph01:~# cephadm shell --name mon.ceph01 --mount /root:/roothome
</span></span><span style=display:flex><span>Inferring fsid f52ba149-31ea-4da9-9b11-17e745e20d35
</span></span><span style=display:flex><span>Inferring config /var/lib/ceph/f52ba149-31ea-4da9-9b11-17e745e20d35/mon.ceph01/config
</span></span><span style=display:flex><span>Using recent ceph image quay.io/ceph/ceph@sha256:1b9158ce28975f95def6a0ad459fa19f1336506074267a4b47c1bd914a00fec0
</span></span><span style=display:flex><span>root@ceph01:/# cd /roothome/
</span></span><span style=display:flex><span>root@ceph01:/roothome# ceph-mon -i ceph01 --extract-monmap monmap-$(date +%F)
</span></span><span style=display:flex><span>2025-11-05T08:56:17.050+0000 70bab6991834 -1 wrote monmap to monmap-2025-11-05
</span></span><span style=display:flex><span>root@ceph01:/roothome# cp monmap-2025-11-05{,-new}
</span></span></code></pre></div><p><strong>Go to another terminal, back up the old monmap now!</strong></p><p>Now we remove the old IPs and add the new ones. I believe the up-to-date version
of the add command more complicated than it needed to be and opted for the old
syntax.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>root@ceph01:/roothome# monmaptool --print monmap-2025-11-05-new
</span></span><span style=display:flex><span>monmaptool: monmap file monmap-2025-11-05-new
</span></span><span style=display:flex><span>epoch 10
</span></span><span style=display:flex><span>fsid f52ba149-31ea-4da9-9b11-17e745e20d35
</span></span><span style=display:flex><span>last_changed 2024-05-12T12:42:35.759936+0000
</span></span><span style=display:flex><span>created 2022-01-13T13:05:50.128859+0000
</span></span><span style=display:flex><span>min_mon_release 18 (reef)
</span></span><span style=display:flex><span>election_strategy: 1
</span></span><span style=display:flex><span>0: [v2:10.10.10.1:3300/0,v1:10.10.10.1:6789/0] mon.ceph01
</span></span><span style=display:flex><span>1: [v2:10.10.10.2:3300/0,v1:10.10.10.2:6789/0] mon.ceph02
</span></span><span style=display:flex><span>2: [v2:10.10.10.3:3300/0,v1:10.10.10.3:6789/0] mon.ceph03
</span></span><span style=display:flex><span>3: [v2:10.10.10.4:3300/0,v1:10.10.10.4:6789/0] mon.ceph04
</span></span><span style=display:flex><span>4: [v2:10.10.10.5:3300/0,v1:10.10.10.5:6789/0] mon.ceph05
</span></span><span style=display:flex><span>root@ceph01:/roothome# monmaptool --rm=ceph{01..05} monmap-2025-11-05-new
</span></span><span style=display:flex><span>monmaptool: monmap file monmap-2025-11-05-new
</span></span><span style=display:flex><span>monmaptool: removing ceph01
</span></span><span style=display:flex><span>monmaptool: removing ceph02
</span></span><span style=display:flex><span>monmaptool: removing ceph03
</span></span><span style=display:flex><span>monmaptool: removing ceph04
</span></span><span style=display:flex><span>monmaptool: removing ceph05
</span></span><span style=display:flex><span>monmaptool: writing epoch 10 to monmap-2025-11-05-new (0 monitors)
</span></span><span style=display:flex><span>root@ceph01:/roothome# monmaptool --add ceph01 192.168.160.1 --add ceph02 192.168.160.2 --add ceph03 192.168.160.3 --add ceph04 192.168.160.4 --add ceph05 192.168.160.5 monmap-2025-11-05-new
</span></span><span style=display:flex><span>monmaptool: monmap file monmap-2025-11-05-new
</span></span><span style=display:flex><span>monmaptool: writing epoch 10 to monmap-2025-11-05-new (6 monitors)
</span></span><span style=display:flex><span>root@ceph01:/roothome# monmaptool --print monmap-2025-11-05-new
</span></span><span style=display:flex><span>monmaptool: monmap file monmap-2025-11-05-new
</span></span><span style=display:flex><span>epoch 10
</span></span><span style=display:flex><span>fsid f52ba149-31ea-4da9-9b11-17e745e20d35
</span></span><span style=display:flex><span>last_changed 2024-05-12T12:42:35.759936+0000
</span></span><span style=display:flex><span>created 2022-01-13T13:05:50.128859+0000
</span></span><span style=display:flex><span>min_mon_release 18 (reef)
</span></span><span style=display:flex><span>election_strategy: 1
</span></span><span style=display:flex><span>0: [v2:192.168.160.1:3300/0,v1:192.168.160.1:6789/0] mon.ceph01
</span></span><span style=display:flex><span>1: [v2:192.168.160.2:3300/0,v1:192.168.160.2:6789/0] mon.ceph02
</span></span><span style=display:flex><span>2: [v2:192.168.160.3:3300/0,v1:192.168.160.3:6789/0] mon.ceph03
</span></span><span style=display:flex><span>3: [v2:192.168.160.4:3300/0,v1:192.168.160.4:6789/0] mon.ceph04
</span></span><span style=display:flex><span>4: [v2:192.168.160.5:3300/0,v1:192.168.160.5:6789/0] mon.ceph05
</span></span></code></pre></div><p>Ensure this is absolutely correct. When I did this I accidentally added another
Ceph Mon that did not exist. Since there were still enough Ceph Mons to have a
quorum, and this change needed to be done during a cluster downtime I was not
worried about temporarily losing quorum if two more Ceph Mons failed or did not
come up right away. I later <a href=https://docs.ceph.com/en/reef/rados/operations/add-or-rm-mons/#removing-a-monitor-manual>removed it</a> after the Ceph Mons
were up again.</p><p>Since I did not want to do these steps for every Ceph Mon host, I copied the new
monmap to the relevant hosts, quick and dirty.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#66d9ef>for</span> i in <span style=color:#f92672>{</span>02..05<span style=color:#f92672>}</span>; <span style=color:#66d9ef>do</span>
</span></span><span style=display:flex><span>    ssh ceph01 sudo cat /root/monmap-2025-11-05-new <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    | ssh ceph$i sudo tee /root/monmap-2025-11-05-new &gt; /dev/null
</span></span><span style=display:flex><span><span style=color:#66d9ef>done</span>
</span></span></code></pre></div><p>I ran the following commands to inject the monmap on each one of them, quick and
dirty as well.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#66d9ef>for</span> i in <span style=color:#f92672>{</span>01..05<span style=color:#f92672>}</span>; <span style=color:#66d9ef>do</span>
</span></span><span style=display:flex><span>    ssh ceph$i <span style=color:#e6db74>&#39;sudo cephadm shell --mount /root:/roothome --name mon.$HOSTNAME ceph-mon -i $HOSTNAME --inject-monmap /roothome/monmap-2025-11-05-new&#39;</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>done</span>
</span></span></code></pre></div><p>I updated <code>/var/lib/ceph/{FSID}/mon.{MON}/config</code> on every host. Either by
replacing the IPs in the config or by running <code>ceph config generate-minimal-conf</code> in <code>cephadm shell</code>.</p><p>So far this was only preparations for the network change. In my case the network
configuration is managed by an Ansible role, and I already made the necessary
changes. This is the point of <del>no return</del> annoying rollback, if one would be
needed for any reason.</p><p>Note: For rolling back we would inject the old monmap for all Ceph Mons. After
changing the network configuration on the hosts and the switches, this might be
harder as we might not be able to easily connect to the hosts to deploy the old
network configuration.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>ansible-playbook site.yaml -t ceph_networking -D
</span></span></code></pre></div><p>Since I am not in charge of the network hardware, this was the time to talk to
the networking people, so they would switch over to the new configuration.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>ansible ceph_hosts -m reboot -b
</span></span></code></pre></div><p>After the reboots I checked the network using another Ansible script, that would
check for bond failures, negotiated speed, VLAN tags, and finally pinging from
every host to every other host on public and cluster network using jumbo frames.
Depending on the cluster size this takes a while but is worth the effort in my
experience to not run into weird issues due to missing MTU on some interfaces or
other configuration errors (hardware or software).</p><p>Note: To ping from everywhere to everywhere it builds a list of IPs filtered by
CIDRs and loops over this list on every host.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>ansible-playbook check-network.yaml
</span></span></code></pre></div><p>Restart the cluster, if everything is okay.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>ansible ceph_hosts -m systemd -a <span style=color:#e6db74>&#39;name=ceph.target state=started enabled=true&#39;</span> -b
</span></span></code></pre></div><p>Setting <code>public_network</code>. I am not sure why it is set on <code>mon</code> and <code>global</code> level
but there is no harm updating both.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph config set mon public_network 192.168.160.128/24
</span></span><span style=display:flex><span>ceph config set global public_network 192.168.160.128/24
</span></span></code></pre></div><p>Fixing Ceph Mgr config.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph config generate-minimal-conf &gt;/var/lib/ceph/f52ba149-31ea-4da9-9b11-17e745e20d35/mgr.ceph01.yeinoh/config
</span></span></code></pre></div><p>In my case I had to <code>systemctl reset-failed 'ceph-*@mgr.ceph*'</code> and start the
service manually. Since the orchestrator does not know the new IPs yet, I needed
to set them.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>ceph orch host set-addr ceph01 192.168.160.1
</span></span><span style=display:flex><span><span style=color:#75715e># repeat for all hosts</span>
</span></span></code></pre></div><p>It will take a few minutes for the orchestrator to gather all information from
every host again.</p><p>The documentation only mentions one last command to reconfigure the Ceph OSDs
before it leaves you with the rest to fix for yourself. For example, to get a
working standby Ceph Mgr and some other services I ran reconfig for all services
on this cluster, including Ceph Mons to ensure there is no residue config
referencing the old network.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>ceph orch reconfig osd
</span></span><span style=display:flex><span>ceph orch reconfig mgr
</span></span><span style=display:flex><span>ceph orch reconfig mds
</span></span><span style=display:flex><span>ceph orch reconfig mon
</span></span><span style=display:flex><span>ceph orch reconfig rgw
</span></span></code></pre></div><p>Note: I guess <code>ceph orch redeploy $service</code> would work as well but would take
longer.</p><p>Now we unset the flags to allow the cluster to function again normally.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>ceph osd unset noout
</span></span><span style=display:flex><span>ceph osd unset norebalance
</span></span><span style=display:flex><span>ceph osd unset nobackfill
</span></span></code></pre></div><p>Now for the supporting services, that need to be checked for functionality and I
will try to include all relevant ones (including unused ones in this case):</p><ul><li>Ceph Mgr should provide Ceph Dashboard with proper certificates and Ceph
Exporter</li><li>Observability tooling should be functional: Prometheus, Node-Exporter, Loki,
Promtail, Grafana</li><li>If you scrape from another external Prometheus you might need to change IPs to
be scraped there</li><li>If there are custom containers, e.g. S3 usage exporter, they should still be
reachable</li><li>Sync services functional: RGW Sync, RBD Mirror, and CephFS Mirror</li><li>Change DNS RR(set)s for RGWs, etc.</li></ul><p>Lastly the RADOS clients (RGW, RBD, CephFS) need to be reconfigured to be able
to communicate with the cluster again.</p><p>And if there are firewalls in place, their rules should be updated <em>beforehand</em>;
on hosts (iptables/nftables) and on centralized firewall appliances.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>While it is possible and fairly straight forward to move a Cluster to another or
multiple other networks, I would advice to avoid the procedure. You might end up
with no cluster at all, if you are not able to fix issues on the way. Even if
you do not make mistakes during the process you might still encounter new
hurdles with later versions or something might fail during the process.</p><p>Understanding how to heal your cluster in an emergency is vital and you should
have the ability to do so, if you attempt this.</p><p>This procedure really should not be a regular occurrence, which is why I did not
automate it the first time and not the second time I had to do it.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://www.relg.uk/>relg.uk</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>