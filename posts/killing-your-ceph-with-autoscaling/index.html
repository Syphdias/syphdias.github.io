<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Killing your Ceph with Autoscaling | relg.uk</title>
<meta name=keywords content><meta name=description content="I recently was consulted on a Ceph Cluster running into nearfull and backfillfull
for the first time. One Ceph OSD was utilized over 85% and another over 90%. The
operators were unaware of the meaning and what to do about it, so took a look.
Looking at ceph status and ceph df, I noticed something. Try to spot it
yourself – I made it easier by removing some stuff around it:"><meta name=author content><link rel=canonical href=https://www.relg.uk/posts/killing-your-ceph-with-autoscaling/><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://www.relg.uk/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.relg.uk/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.relg.uk/favicon-32x32.png><link rel=apple-touch-icon href=https://www.relg.uk/apple-touch-icon.png><link rel=mask-icon href=https://www.relg.uk/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://www.relg.uk/posts/killing-your-ceph-with-autoscaling/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://www.relg.uk/posts/killing-your-ceph-with-autoscaling/"><meta property="og:site_name" content="relg.uk"><meta property="og:title" content="Killing your Ceph with Autoscaling"><meta property="og:description" content="I recently was consulted on a Ceph Cluster running into nearfull and backfillfull for the first time. One Ceph OSD was utilized over 85% and another over 90%. The operators were unaware of the meaning and what to do about it, so took a look.
Looking at ceph status and ceph df, I noticed something. Try to spot it yourself – I made it easier by removing some stuff around it:"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-01-11T22:42:15+01:00"><meta property="article:modified_time" content="2023-01-11T22:42:15+01:00"><meta name=fediverse:creator content="@syphdias@social.linux.pizza"><meta name=twitter:card content="summary"><meta name=twitter:title content="Killing your Ceph with Autoscaling"><meta name=twitter:description content="I recently was consulted on a Ceph Cluster running into nearfull and backfillfull
for the first time. One Ceph OSD was utilized over 85% and another over 90%. The
operators were unaware of the meaning and what to do about it, so took a look.
Looking at ceph status and ceph df, I noticed something. Try to spot it
yourself – I made it easier by removing some stuff around it:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.relg.uk/posts/"},{"@type":"ListItem","position":2,"name":"Killing your Ceph with Autoscaling","item":"https://www.relg.uk/posts/killing-your-ceph-with-autoscaling/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Killing your Ceph with Autoscaling","name":"Killing your Ceph with Autoscaling","description":"I recently was consulted on a Ceph Cluster running into nearfull and backfillfull for the first time. One Ceph OSD was utilized over 85% and another over 90%. The operators were unaware of the meaning and what to do about it, so took a look.\nLooking at ceph status and ceph df, I noticed something. Try to spot it yourself – I made it easier by removing some stuff around it:\n","keywords":[],"articleBody":"I recently was consulted on a Ceph Cluster running into nearfull and backfillfull for the first time. One Ceph OSD was utilized over 85% and another over 90%. The operators were unaware of the meaning and what to do about it, so took a look.\nLooking at ceph status and ceph df, I noticed something. Try to spot it yourself – I made it easier by removing some stuff around it:\n$ ceph status [...] health: HEALTH_WARN 1 pools have many more objects per pg than average 1 backfillfull osd(s) 1 nearfull osd(s) Low space hindering backfill (add storage if this doesn't resolve itself): 1 pg backfill_toofull 1 pgs not deep-scrubbed in time 1 pgs not scrubbed in time 20 pool(s) backfillfull services: [...] osd: 96 osds: 96 up (since 4w), 96 in (since 12M); 1 remapped pgs [...] data: volumes: 4/4 healthy pools: 20 pools, 4769 pgs objects: 31.90M objects, 117 TiB usage: 351 TiB used, 522 TiB / 873 TiB avail pgs: 299136/95689743 objects misplaced (0.313%) 4763 active+clean 5 active+clean+scrubbing+deep 1 active+remapped+backfill_toofull [...] # ceph df --- RAW STORAGE --- CLASS SIZE AVAIL USED RAW USED %RAW USED hdd 873 TiB 522 TiB 351 TiB 351 TiB 40.19 TOTAL 873 TiB 522 TiB 351 TiB 351 TiB 40.19 --- POOLS --- POOL ID PGS STORED OBJECTS USED %USED MAX AVAIL device_health_metrics 1 4096 907 MiB 108 2.7 GiB 0 14 TiB rbd 4 145 97 TiB 25.39M 290 TiB 87.43 14 TiB [...] The raw usage was only at 40%. Why would one disk contain so much data? The balancer was in upmap mode and active. But even with no balancer, this kind of miss-balancing would be extreme and very unlikely.\nYou may have already spotted something odd in the Ceph Pool configuration. While device_health_metrics contained less than 1GiB, it had 4096 PGs. At the same time rbd contained 97TiB in just 145 PGs.\n145 is not just no power of two (which would usually produce a Ceph Warning), but also way to low for the about of data and Ceph OSD count.\nWhat Does This Mean for Storage Distribution? Estimating the size of one PG for pool rbd yields about 685GiB (97TiB/145). How many (average sized) PGs will lead to utilization of one disk over 85%?\nAbout 11.4 (85% * 9TiB / 685GiB)\nUnfortunately, not every PG is the same size. Looking at the sizes, multiple PG exceed 800GiB. Furthermore not every Ceph OSD receives the same amount of PGs. And as we will soon see the number of PGs was trying to get lower.\nCause and Distributing Data But what actually caused the bogus PG numbers? The answer is: The PG Autoscaler. For some reason only device_health_metrics set a target_size_ratio to 0.1. This lead to the effective ratio to be 1 for this pool. Apparently the autoscaling assumed this would mean all data would be stored in this pool. This also explained why the number of PGs was not a power of two. The autoscaler set target_pg_num to 32 to reduce the pool rbd even more. This was why there was not Ceph Warning. This also means that if there were no disks were running full right now, it certainly would have happened in the following days.\nBefore removing the ratio, I wanted to know what would happen. I disabled autoscaling (ceph osd pool set noautoscale) and removed the target ratio:\n# ceph osd pool set device_health_metrics target_size_ratio 0 # ceph osd pool autoscale-status POOL SIZE TARGET SIZE RATE RAW CAPACITY RATIO TARGET RATIO EFFECTIVE RATIO BIAS PG_NUM NEW PG_NUM AUTOSCALE BULK device_health_metrics 906.5M 3.0 873.1T 0.0000 1.0 4096 1 on False rbd 99048G 3.0 873.1T 0.3323 1.0 32 1024 on False [...] This was a lot better and we decided to re-enable autoscaling (ceph osd pool unset noautoscale) right away.\nAfter a few minutes the backfillfull was gone. Soon to be followed by the nearfull. After a few days of rebalancing both pools had the proper PG count.\nI am not sure why the ratio was set and why it was interpreted as it was. The docs suggest this would not be a problem and I could not reproduce the behavior in a more recent version of Ceph. So this was potentially fixed already.\n","wordCount":"709","inLanguage":"en","datePublished":"2023-01-11T22:42:15+01:00","dateModified":"2023-01-11T22:42:15+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.relg.uk/posts/killing-your-ceph-with-autoscaling/"},"publisher":{"@type":"Organization","name":"relg.uk","logo":{"@type":"ImageObject","url":"https://www.relg.uk/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark"),document.addEventListener("DOMContentLoaded",function(){sessionStorage.getItem("nthLoad")?(document.getElementById("logo").classList.remove("logo-first-load"),document.getElementById("logo").classList.add("logo-nth-load")):sessionStorage.setItem("nthLoad","true")})</script><style>.logo{display:flex;font-family:MesloLGS NF,monospace;font-size:2em;cursor:pointer}.logo span{display:inline-block}.logo-first-load span{animation-delay:1s}@keyframes char1{0%{transform:translateX(0)}100%{transform:translateX(5ch)}}@keyframes char2{0%{transform:translateX(0)}100%{transform:translateX(3ch)}}@keyframes char3{0%{transform:translateX(0)}100%{transform:translateX(1ch)}}@keyframes char4{0%{transform:translateX(0)}100%{transform:translateX(-1ch)}}@keyframes char5{0%{transform:translateX(0);opacity:1}100%{transform:translateX(-2ch);opacity:0}}@keyframes char6{0%{transform:translateX(0)}100%{transform:translateX(-4ch)}}@keyframes char7{0%{transform:translateX(0)}100%{transform:translateX(-6ch)}}.logo-first-load span:nth-child(1){animation:char1 .5s ease 1s forwards}.logo-first-load span:nth-child(2){animation:char2 .5s ease 1s forwards}.logo-first-load span:nth-child(3){animation:char3 .5s ease 1s forwards}.logo-first-load span:nth-child(4){animation:char4 .5s ease 1s forwards}.logo-first-load span:nth-child(5){animation:char5 .5s ease 1s forwards}.logo-first-load span:nth-child(6){animation:char6 .5s ease 1s forwards}.logo-first-load span:nth-child(7){animation:char7 .5s ease 1s forwards}@keyframes char1r{0%{transform:translateX(5ch)}100%{transform:translateX(0)}}@keyframes char2r{0%{transform:translateX(3ch)}100%{transform:translateX(0)}}@keyframes char3r{0%{transform:translateX(1ch)}100%{transform:translateX(0)}}@keyframes char4r{0%{transform:translateX(-1ch)}100%{transform:translateX(0)}}@keyframes char5r{0%{transform:translateX(-2ch);opacity:0}100%{transform:translateX(0);opacity:1}}@keyframes char6r{0%{transform:translateX(-4ch)}100%{transform:translateX(0)}}@keyframes char7r{0%{transform:translateX(-6ch)}100%{transform:translateX(0)}}.logo-first-load:hover span{animation-delay:0s}.logo-first-load:hover span:nth-child(1){animation-name:char1r}.logo-first-load:hover span:nth-child(2){animation-name:char2r}.logo-first-load:hover span:nth-child(3){animation-name:char3r}.logo-first-load:hover span:nth-child(4){animation-name:char4r}.logo-first-load:hover span:nth-child(5){animation-name:char5r}.logo-first-load:hover span:nth-child(6){animation-name:char6r}.logo-first-load:hover span:nth-child(7){animation-name:char7r}.logo-nth-load span{transition:transform .5s ease,opacity .5s ease}.logo-nth-load:hover span:nth-child(1){transform:translateX(5ch)}.logo-nth-load:hover span:nth-child(2){transform:translateX(3ch)}.logo-nth-load:hover span:nth-child(3){transform:translateX(1ch)}.logo-nth-load:hover span:nth-child(4){transform:translateX(-1ch)}.logo-nth-load:hover span:nth-child(5){transform:translateX(-2ch);opacity:0}.logo-nth-load:hover span:nth-child(6){transform:translateX(-4ch)}.logo-nth-load:hover span:nth-child(7){transform:translateX(-6ch)}</style><header class=header><nav class=nav><div id=logo class="logo logo-first-load"><a href=https://www.relg.uk/ accesskey=h title="relg.uk (Alt + H)"><span>r</span><span>e</span><span>l</span><span>g</span><span>.</span><span>u</span><span>k</span></a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.relg.uk/search/ title="search (Alt + /)" accesskey=/><span><img src=/magnifying-glass-solid.svg alt=search style=height:1em;width:1em;display:inline-block;margin-right:.3em;vertical-align:middle>
search</span></a></li><li><a href=https://www.relg.uk/archives/ title=archive><span><img src=/box-archive-solid.svg alt=archive style=height:1em;width:1em;display:inline-block;margin-right:.3em;vertical-align:middle>
archive</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Killing your Ceph with Autoscaling</h1><div class=post-meta><span title='2023-01-11 22:42:15 +0100 +0100'>2023-01-11</span>&nbsp;·&nbsp;4 min&nbsp;|&nbsp;<a href=https://github.com/syphdias/syphdias.github.io/edit/main/content/posts/killing-your-ceph-with-autoscaling.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>I recently was consulted on a Ceph Cluster running into nearfull and backfillfull
for the first time. One Ceph OSD was utilized over 85% and another over 90%. The
operators were unaware of the meaning and what to do about it, so took a look.</p><p>Looking at <code>ceph status</code> and <code>ceph df</code>, I noticed something. Try to spot it
yourself – I made it easier by removing some stuff around it:</p><pre tabindex=0><code>$ ceph status
[...]
    health: HEALTH_WARN
            1 pools have many more objects per pg than average
            1 backfillfull osd(s)
            1 nearfull osd(s)
            Low space hindering backfill (add storage if this doesn&#39;t resolve itself): 1 pg backfill_toofull
            1 pgs not deep-scrubbed in time
            1 pgs not scrubbed in time
            20 pool(s) backfillfull

  services:
[...]
    osd: 96 osds: 96 up (since 4w), 96 in (since 12M); 1 remapped pgs
[...]
  data:
    volumes: 4/4 healthy
    pools:   20 pools, 4769 pgs
    objects: 31.90M objects, 117 TiB
    usage:   351 TiB used, 522 TiB / 873 TiB avail
    pgs:     299136/95689743 objects misplaced (0.313%)
             4763 active+clean
             5    active+clean+scrubbing+deep
             1    active+remapped+backfill_toofull
[...]
# ceph df
--- RAW STORAGE ---
CLASS     SIZE    AVAIL     USED  RAW USED  %RAW USED
hdd    873 TiB  522 TiB  351 TiB   351 TiB      40.19
TOTAL  873 TiB  522 TiB  351 TiB   351 TiB      40.19

--- POOLS ---
POOL                           ID   PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
device_health_metrics           1  4096  907 MiB      108  2.7 GiB      0     14 TiB
rbd                             4   145   97 TiB   25.39M  290 TiB  87.43     14 TiB
[...]
</code></pre><p>The raw usage was only at 40%. Why would one disk contain so much data? The
balancer was in upmap mode and active. But even with no balancer, this kind of
miss-balancing would be extreme and very unlikely.</p><p>You may have already spotted something odd in the Ceph Pool configuration. While
<code>device_health_metrics</code> contained less than 1GiB, it had 4096 PGs. At the same
time <code>rbd</code> contained 97TiB in just 145 PGs.</p><p>145 is not just no power of two (which would usually produce a Ceph Warning),
but also way to low for the about of data and Ceph OSD count.</p><h2 id=what-does-this-mean-for-storage-distribution>What Does This Mean for Storage Distribution?<a hidden class=anchor aria-hidden=true href=#what-does-this-mean-for-storage-distribution>#</a></h2><p>Estimating the size of one PG for pool <code>rbd</code> yields about 685GiB (97TiB/145).
How many (average sized) PGs will lead to utilization of one disk over 85%?</p><p>About 11.4 (85% * 9TiB / 685GiB)</p><p>Unfortunately, not every PG is the same size. Looking at the sizes, multiple PG
exceed 800GiB. Furthermore not every Ceph OSD receives the same amount of PGs.
And as we will soon see the number of PGs was trying to get lower.</p><h2 id=cause-and-distributing-data>Cause and Distributing Data<a hidden class=anchor aria-hidden=true href=#cause-and-distributing-data>#</a></h2><p>But what actually caused the bogus PG numbers? The answer is: The PG Autoscaler.
For some reason only <code>device_health_metrics</code> set a <code>target_size_ratio</code> to 0.1.
This lead to the effective ratio to be 1 for this pool. Apparently the autoscaling
assumed this would mean all data would be stored in this pool. This also
explained why the number of PGs was not a power of two. The autoscaler set
<code>target_pg_num</code> to 32 to reduce the pool <code>rbd</code> even more. This was why there was
not Ceph Warning. This also means that if there were no disks were running full
right now, it certainly would have happened in the following days.</p><p>Before removing the ratio, I wanted to know what would happen. I disabled
autoscaling (<code>ceph osd pool set noautoscale</code>) and removed the target ratio:</p><pre tabindex=0><code># ceph osd pool set device_health_metrics target_size_ratio 0
# ceph osd pool autoscale-status
POOL                             SIZE  TARGET SIZE  RATE  RAW CAPACITY   RATIO  TARGET RATIO  EFFECTIVE RATIO  BIAS  PG_NUM  NEW PG_NUM  AUTOSCALE  BULK
device_health_metrics          906.5M                3.0        873.1T  0.0000                                  1.0    4096           1  on         False
rbd                            99048G                3.0        873.1T  0.3323                                  1.0      32        1024  on         False
[...]
</code></pre><p>This was a lot better and we decided to re-enable autoscaling (<code>ceph osd pool unset noautoscale</code>) right away.</p><p>After a few minutes the backfillfull was gone. Soon to be followed by the
nearfull. After a few days of rebalancing both pools had the proper PG count.</p><p>I am not sure why the ratio was set and why it was interpreted as it was. The
<a href=https://docs.ceph.com/en/latest/rados/operations/placement-groups/#viewing-pg-scaling-recommendations>docs</a>
suggest this would not be a problem and I could not reproduce the behavior in a
more recent version of Ceph. So this was potentially fixed already.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://www.relg.uk/>relg.uk</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>