<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Killing Ceph by Filling It | relg.uk</title>
<meta name=keywords content><meta name=description content="Apparently I do not have any customers that do not fill their Ceph clusters to
the brim. Either because the lack capacity management, do not know their usage
(patterns), or do not buy more storage nodes — for one or another reason.
This leaves me in the situation to help get the cluster in order.
Disclaimer
This is not a in depth guide for which steps to take exactly but a jumping off
point to do your own reading in the Ceph documentation and acting accordingly.
When in doubt, reach out to professionals, e.g. on the Mailing List or in IRC."><meta name=author content><link rel=canonical href=https://www.relg.uk/posts/killing-ceph-by-filling-it/><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://www.relg.uk/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.relg.uk/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://www.relg.uk/favicon-32x32.png><link rel=apple-touch-icon href=https://www.relg.uk/apple-touch-icon.png><link rel=mask-icon href=https://www.relg.uk/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://www.relg.uk/posts/killing-ceph-by-filling-it/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://www.relg.uk/posts/killing-ceph-by-filling-it/"><meta property="og:site_name" content="relg.uk"><meta property="og:title" content="Killing Ceph by Filling It"><meta property="og:description" content="Apparently I do not have any customers that do not fill their Ceph clusters to the brim. Either because the lack capacity management, do not know their usage (patterns), or do not buy more storage nodes — for one or another reason.
This leaves me in the situation to help get the cluster in order.
Disclaimer This is not a in depth guide for which steps to take exactly but a jumping off point to do your own reading in the Ceph documentation and acting accordingly. When in doubt, reach out to professionals, e.g. on the Mailing List or in IRC."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-21T19:20:34+01:00"><meta property="article:modified_time" content="2025-01-21T19:20:34+01:00"><meta name=fediverse:creator content="@syphdias@social.linux.pizza"><meta name=twitter:card content="summary"><meta name=twitter:title content="Killing Ceph by Filling It"><meta name=twitter:description content="Apparently I do not have any customers that do not fill their Ceph clusters to
the brim. Either because the lack capacity management, do not know their usage
(patterns), or do not buy more storage nodes — for one or another reason.
This leaves me in the situation to help get the cluster in order.
Disclaimer
This is not a in depth guide for which steps to take exactly but a jumping off
point to do your own reading in the Ceph documentation and acting accordingly.
When in doubt, reach out to professionals, e.g. on the Mailing List or in IRC."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.relg.uk/posts/"},{"@type":"ListItem","position":2,"name":"Killing Ceph by Filling It","item":"https://www.relg.uk/posts/killing-ceph-by-filling-it/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Killing Ceph by Filling It","name":"Killing Ceph by Filling It","description":"Apparently I do not have any customers that do not fill their Ceph clusters to the brim. Either because the lack capacity management, do not know their usage (patterns), or do not buy more storage nodes — for one or another reason.\nThis leaves me in the situation to help get the cluster in order.\nDisclaimer This is not a in depth guide for which steps to take exactly but a jumping off point to do your own reading in the Ceph documentation and acting accordingly. When in doubt, reach out to professionals, e.g. on the Mailing List or in IRC.\n","keywords":[],"articleBody":"Apparently I do not have any customers that do not fill their Ceph clusters to the brim. Either because the lack capacity management, do not know their usage (patterns), or do not buy more storage nodes — for one or another reason.\nThis leaves me in the situation to help get the cluster in order.\nDisclaimer This is not a in depth guide for which steps to take exactly but a jumping off point to do your own reading in the Ceph documentation and acting accordingly. When in doubt, reach out to professionals, e.g. on the Mailing List or in IRC.\nObvious Things to Act on Assuming we are not in a critical state yet, where we can no longer use the cluster. A few steps are fairly obvious but worth reiterating:\nUnderstand where data is coming from Stop the ingress of new data Delete unneeded data (also unneeded snapshots) After that you can take a look at how to prevent the cluster filling up in the mid to long term, e.g. retention mechanisms, etc.\nCeph’s Balancing Mechanisms If the cluster reports “nearfull” and “backfillfull” there is still time and you should check how well balanced your cluster is. Due to how RADOS objects are placed into Placement Groups (PG) using CRUSH PGs can differ — in some cases quite widely — in size. The PGs are being assigned to Ceph OSDs and due to chance you end up with one Ceph OSDs containing multiple large PGs and another Ceph OSD containing only small PGs. In an ideal world every PG and Ceph OSD would have the same size and the distribution would even and would grow even. But, alas, PG sizes vary (especially between pools), Ceph OSDs have different sizes and the storage usage is unevenly distributed. To fight this, Ceph has the Balancer Module that tries to do its best to optimize a low spread of different usages. Make sure it is enabled and used “upmap”.\nceph balancer status Another variable that can be tuned automatically by Ceph is PGs per pool. It takes in some variables like pool usage, target size and target ratio to calculate the ideal pg_num count for the pools it is enabled on. I recommend to at least put this on warn for every pool, if you have no reason not to. I did have some issues in the past with it but nonetheless it is worth at least getting some advice from it.\nceph osd pool autoscale-status The autoscaler is a good starting point especially, if you do not know the exact distribution of your data at the beginning. In the past PG count always had to be calculated manually with a target count of approximately 100 PGs per OSD. There is PG Calc to do this manually and it is still useful today.\nIdeally you know your target pool sizes or at least ratios and set the values accordingly for each pool and let the autoscaler do the rest. Or you can set the bulk flag for your data pools.\nIt is worth checking the PG distribution yourself with PG Calc and see if you have enough PGs per OSD. If you have only about 50 PGs per OSD increasing the PG count can help achieve better balancing and faster recovery times. The reasons for this are fairly simple: If you have more and smaller PGs the likelihood of one PG getting larger is lower and the chance of multiple large PGs ending up on one OSD is lower as well.\nLooking at ceph osd df, and ceph balancer eval can give you an idea how well your data is distributed.\nExample of Increasing PG Counts This is from a purely replicated RADOS cluster with HDDs OSDs only (except for the RocksDB).\nBefore:\n❯ ceph balancer eval current cluster score 0.041072 (lower is better) ❯ # awk filters relevant information: min/max OSD, total, PG/OSD, and variance ❯ ceph osd df |awk -v min=100 -v max=0 \\ '/VAR/ {print} /hdd/ { if($17max) {max_line=$0;max=$17} } /TOTAL/ { print min_line; print max_line; print \"Total:\", sum, \"TiB\"; print \"PG/OSD:\", pg/c }' ID CLASS WEIGHT REWEIGHT SIZE RAW USE DATA OMAP META AVAIL %USE VAR PGS STATUS 129 hdd 9.27039 1.00000 9.3 TiB 4.2 TiB 4.0 TiB 4.6 MiB 8.8 GiB 5.1 TiB 45.30 0.73 48 up 62 hdd 9.27039 1.00000 9.3 TiB 7.4 TiB 7.2 TiB 6.3 MiB 18 GiB 1.9 TiB 79.65 1.29 64 up Total: 1374.5 TiB PG/OSD: 57.8458 MIN/MAX VAR: 0.73/1.29 STDDEV: 6.47 All pools have been set to ceph osd pool set pg_autoscale_mode warn and some of them have been assigned the bulk flag (ceph osd pool set bulk true) to make the autoscaler expect more data in this pool (usually your RBD pool, RGW data pool, or CephFS data pool).\nAfter:\n❯ ceph balancer eval current cluster score 0.025255 (lower is better) ❯ # awk filters relevant information: min/max OSD, total, PG/OSD, and variance ❯ ceph osd df |awk -v min=100 -v max=0 \\ '/VAR/ {print} /hdd/ { sum+=$7; c+=1; pg+=$19; if($17max) {max_line=$0;max=$17} } /TOTAL/ { print min_line; print max_line; print \"Total:\", sum, \"TiB\"; print \"PG/OSD:\", pg/c }' ID CLASS WEIGHT REWEIGHT SIZE RAW USE DATA OMAP META AVAIL %USE VAR PGS STATUS 56 hdd 9.27039 1.00000 9.3 TiB 4.9 TiB 4.7 TiB 1.2 MiB 10 GiB 4.4 TiB 52.47 0.86 76 up 170 hdd 9.27039 1.00000 9.3 TiB 6.5 TiB 6.3 TiB 1.1 MiB 13 GiB 2.8 TiB 70.20 1.16 98 up Total: 1351.3 TiB PG/OSD: 86.6458 MIN/MAX VAR: 0.86/1.16 STDDEV: 3.42 After increasing pg_num for two pools and waiting for the process to finish (took about half a week), you can see that even though the total size did decreased a little bit (~20TiB) the variance shown by ceph balancer eval and the standard deviation shown by ceph osd df went down. This is also reflected by the fullest OSD being significantly lower (79% -\u003e 70%) and the least filled OSD being better utilised (45% -\u003e 52%).\nNote that you should also let the balancer do its job after increasing the PG count; so this could even improve a little bit.\nYou can see with more PGs the spread of OSD capacities narrows and focuses more on a lower number which is exactly what you want to see. Interestingly, if you look at the PGs per OSD you can also see that after the initial increase in the count of PGs the distribution of PGs decreases its spread as well. This is due to the balancer now getting the chance after the data splits to optimize their placements.\nSlight warning: Do not be alarmed if you get warning about (deep-)scrubs not being done on time. Since increasing PG count leads to lots of backfilling potentially over a prolonged period, Ceph prioritizes backfilling over scrubs. After the cluster is back to a static number of PGs and finished all backfilling task the number of scrubs not done on time should decrease (I would expect it to take about as long as the PG increase or less).\nTo improve recovery speed I set ceph config set osd osd_mclock_profile high_recovery_ops. To not impact daily usage too much, I set up an ad-hoc systemd-timers:\nsystemd-run --on-calendar 'Mon..Fri 2024-12-* 18:00' \\ ceph config set osd osd_mclock_profile high_recovery_ops systemd-run --on-calendar 'Mon..Fri 2024-12-* 08:00' \\ ceph config set osd osd_mclock_profile high_client_ops systemd-run --on-calendar '2025-01-01 08:00' \\ ceph config set osd osd_mclock_profile high_client_ops Note: Keep in mind, that if the server these commands were run on restarts, the timers are gone. So make sure to check that you have indeed set the proper osd_mclock_profile after you are done. You could also remove the setting to go back to the balance default with ceph config rm osd osd_mclock_profile\nGetting Your Hands Dirty If this is still not enough there is more you can do, by adjusting the Ceph OSD (re)weights.\nThe existing scripts only looked at the current utilization and acted upon this information. Then you had to wait for the result and tweak another value. I wrote a few scripts for looking at changes during backfilling and adjusting based on future balance results.\nI created a repo, if you want to check them out.\nEmergency Only If you are really in a pickle and need to get a blocked cluster running again to be able to rebalance (not to use it!), you can change what is recognized as a backfillfull or full Ceph OSD. I do not recommend this!\nceph osd dump | grep ratio ceph osd set-backfillfull-ratio $higher ceph osd set-full-ratio $more_higher Also, make sure to change it back after your incident!\nceph osd set-backfillfull-ratio .9 ceph osd set-full-ratio .95 ceph osd dump | grep ratio ","wordCount":"1455","inLanguage":"en","datePublished":"2025-01-21T19:20:34+01:00","dateModified":"2025-01-21T19:20:34+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.relg.uk/posts/killing-ceph-by-filling-it/"},"publisher":{"@type":"Organization","name":"relg.uk","logo":{"@type":"ImageObject","url":"https://www.relg.uk/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark"),document.addEventListener("DOMContentLoaded",function(){sessionStorage.getItem("nthLoad")?(document.getElementById("logo").classList.remove("logo-first-load"),document.getElementById("logo").classList.add("logo-nth-load")):sessionStorage.setItem("nthLoad","true")})</script><style>.logo{display:flex;font-family:MesloLGS NF,monospace;font-size:2em;cursor:pointer}.logo span{display:inline-block}.logo-first-load span{animation-delay:1s}@keyframes char1{0%{transform:translateX(0)}100%{transform:translateX(5ch)}}@keyframes char2{0%{transform:translateX(0)}100%{transform:translateX(3ch)}}@keyframes char3{0%{transform:translateX(0)}100%{transform:translateX(1ch)}}@keyframes char4{0%{transform:translateX(0)}100%{transform:translateX(-1ch)}}@keyframes char5{0%{transform:translateX(0);opacity:1}100%{transform:translateX(-2ch);opacity:0}}@keyframes char6{0%{transform:translateX(0)}100%{transform:translateX(-4ch)}}@keyframes char7{0%{transform:translateX(0)}100%{transform:translateX(-6ch)}}.logo-first-load span:nth-child(1){animation:char1 .5s ease 1s forwards}.logo-first-load span:nth-child(2){animation:char2 .5s ease 1s forwards}.logo-first-load span:nth-child(3){animation:char3 .5s ease 1s forwards}.logo-first-load span:nth-child(4){animation:char4 .5s ease 1s forwards}.logo-first-load span:nth-child(5){animation:char5 .5s ease 1s forwards}.logo-first-load span:nth-child(6){animation:char6 .5s ease 1s forwards}.logo-first-load span:nth-child(7){animation:char7 .5s ease 1s forwards}@keyframes char1r{0%{transform:translateX(5ch)}100%{transform:translateX(0)}}@keyframes char2r{0%{transform:translateX(3ch)}100%{transform:translateX(0)}}@keyframes char3r{0%{transform:translateX(1ch)}100%{transform:translateX(0)}}@keyframes char4r{0%{transform:translateX(-1ch)}100%{transform:translateX(0)}}@keyframes char5r{0%{transform:translateX(-2ch);opacity:0}100%{transform:translateX(0);opacity:1}}@keyframes char6r{0%{transform:translateX(-4ch)}100%{transform:translateX(0)}}@keyframes char7r{0%{transform:translateX(-6ch)}100%{transform:translateX(0)}}.logo-first-load:hover span{animation-delay:0s}.logo-first-load:hover span:nth-child(1){animation-name:char1r}.logo-first-load:hover span:nth-child(2){animation-name:char2r}.logo-first-load:hover span:nth-child(3){animation-name:char3r}.logo-first-load:hover span:nth-child(4){animation-name:char4r}.logo-first-load:hover span:nth-child(5){animation-name:char5r}.logo-first-load:hover span:nth-child(6){animation-name:char6r}.logo-first-load:hover span:nth-child(7){animation-name:char7r}.logo-nth-load span{transition:transform .5s ease,opacity .5s ease}.logo-nth-load:hover span:nth-child(1){transform:translateX(5ch)}.logo-nth-load:hover span:nth-child(2){transform:translateX(3ch)}.logo-nth-load:hover span:nth-child(3){transform:translateX(1ch)}.logo-nth-load:hover span:nth-child(4){transform:translateX(-1ch)}.logo-nth-load:hover span:nth-child(5){transform:translateX(-2ch);opacity:0}.logo-nth-load:hover span:nth-child(6){transform:translateX(-4ch)}.logo-nth-load:hover span:nth-child(7){transform:translateX(-6ch)}</style><header class=header><nav class=nav><div id=logo class="logo logo-first-load"><a href=https://www.relg.uk/ accesskey=h title="relg.uk (Alt + H)"><span>r</span><span>e</span><span>l</span><span>g</span><span>.</span><span>u</span><span>k</span></a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://www.relg.uk/search/ title="search (Alt + /)" accesskey=/><span><img src=/magnifying-glass-solid.svg alt=search style=height:1em;width:1em;display:inline-block;margin-right:.3em;vertical-align:middle>
search</span></a></li><li><a href=https://www.relg.uk/archives/ title=archive><span><img src=/box-archive-solid.svg alt=archive style=height:1em;width:1em;display:inline-block;margin-right:.3em;vertical-align:middle>
archive</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Killing Ceph by Filling It</h1><div class=post-meta><span title='2025-01-21 19:20:34 +0100 +0100'>2025-01-21</span>&nbsp;·&nbsp;7 min&nbsp;|&nbsp;<a href=https://github.com/syphdias/syphdias.github.io/edit/main/content/posts/killing-ceph-by-filling-it.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>Apparently I do not have any customers that do not fill their Ceph clusters to
the brim. Either because the lack capacity management, do not know their usage
(patterns), or do not buy more storage nodes — for one or another reason.</p><p>This leaves me in the situation to help get the cluster in order.</p><h2 id=disclaimer>Disclaimer<a hidden class=anchor aria-hidden=true href=#disclaimer>#</a></h2><p>This is not a in depth guide for which steps to take exactly but a jumping off
point to do your own reading in the Ceph documentation and acting accordingly.
When in doubt, reach out to professionals, e.g. on the Mailing List or in IRC.</p><h2 id=obvious-things-to-act-on>Obvious Things to Act on<a hidden class=anchor aria-hidden=true href=#obvious-things-to-act-on>#</a></h2><p>Assuming we are not in a critical state yet, where we can no longer use the
cluster. A few steps are fairly obvious but worth reiterating:</p><ol><li>Understand where data is coming from</li><li>Stop the ingress of new data</li><li>Delete unneeded data (also unneeded snapshots)</li></ol><p>After that you can take a look at how to prevent the cluster filling up in the
mid to long term, e.g. retention mechanisms, etc.</p><h2 id=cephs-balancing-mechanisms>Ceph&rsquo;s Balancing Mechanisms<a hidden class=anchor aria-hidden=true href=#cephs-balancing-mechanisms>#</a></h2><p>If the cluster reports &ldquo;nearfull&rdquo; and &ldquo;backfillfull&rdquo; there is still time and you
should check how well balanced your cluster is. Due to how RADOS objects are
placed into Placement Groups (PG) using <a href=https://docs.ceph.com/en/latest/rados/operations/crush-map/>CRUSH</a> PGs can differ — in some cases
quite widely — in size. The PGs are being assigned to Ceph OSDs and due to
chance you end up with one Ceph OSDs containing multiple large PGs and another
Ceph OSD containing only small PGs. In an ideal world every PG and Ceph OSD
would have the same size and the distribution would even and would grow even.
But, alas, PG sizes vary (especially between pools), Ceph OSDs have different
sizes and the storage usage is unevenly distributed. To fight this, Ceph has the
<a href=https://docs.ceph.com/en/latest/rados/operations/balancer/>Balancer Module</a> that tries to do its best to optimize a low spread of
different usages. Make sure it is enabled and used &ldquo;upmap&rdquo;.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>ceph balancer status
</span></span></code></pre></div><p>Another variable that can be tuned automatically by Ceph is PGs per pool. It
takes in some variables like pool usage, target size and target ratio to
calculate the ideal <code>pg_num</code> count for the pools it is enabled on. I recommend
to at least put this on <code>warn</code> for every pool, if you have no reason not to.
I did have <a href=/posts/killing-your-ceph-with-autoscaling/>some issues</a> in the past with it but nonetheless it is worth at
least getting some advice from it.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>ceph osd pool autoscale-status
</span></span></code></pre></div><p>The autoscaler is a good starting point especially, if you do not know the exact
distribution of your data at the beginning. In the past PG count always had to
be calculated manually with a target count of approximately 100 PGs per OSD.
There is <a href=https://docs.ceph.com/en/latest/rados/operations/pgcalc/>PG Calc</a> to do this manually and it is still useful today.</p><p>Ideally you know your target pool sizes or at least ratios and set the values
accordingly for each pool and let the autoscaler do the rest. Or you can set the
bulk flag for your data pools.</p><p>It is worth checking the PG distribution yourself with <a href=https://docs.ceph.com/en/latest/rados/operations/pgcalc/>PG Calc</a> and see if you
have enough PGs per OSD. If you have only about 50 PGs per OSD increasing the PG
count can help achieve better balancing and faster recovery times. The reasons
for this are fairly simple: If you have more and smaller PGs the likelihood of
one PG getting larger is lower and the chance of multiple large PGs ending up on
one OSD is lower as well.</p><p>Looking at <code>ceph osd df</code>, and <code>ceph balancer eval</code> can give you an idea how well
your data is distributed.</p><h3 id=example-of-increasing-pg-counts>Example of Increasing PG Counts<a hidden class=anchor aria-hidden=true href=#example-of-increasing-pg-counts>#</a></h3><p>This is from a purely replicated RADOS cluster with HDDs OSDs only (except for
the RocksDB).</p><p><strong>Before:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>❯ ceph balancer eval
</span></span><span style=display:flex><span>current cluster score 0.041072 (lower is better)
</span></span><span style=display:flex><span>❯ # awk filters relevant information: min/max OSD, total, PG/OSD, and variance
</span></span><span style=display:flex><span>❯ ceph osd df |awk -v min=100 -v max=0 \
</span></span><span style=display:flex><span>    &#39;/VAR/ {print}
</span></span><span style=display:flex><span>     /hdd/ { if($17&lt;min){min_line=$0;min=$17}; if($17&gt;max) {max_line=$0;max=$17} }
</span></span><span style=display:flex><span>     /TOTAL/ { print min_line; print max_line; print &#34;Total:&#34;, sum, &#34;TiB&#34;; print &#34;PG/OSD:&#34;, pg/c }&#39;
</span></span><span style=display:flex><span>ID   CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP     META     AVAIL    %USE   VAR   PGS  STATUS
</span></span><span style=display:flex><span>129    hdd  9.27039   1.00000  9.3 TiB  4.2 TiB  4.0 TiB  4.6 MiB  8.8 GiB  5.1 TiB  45.30  0.73   48      up
</span></span><span style=display:flex><span> 62    hdd  9.27039   1.00000  9.3 TiB  7.4 TiB  7.2 TiB  6.3 MiB   18 GiB  1.9 TiB  79.65  1.29   64      up
</span></span><span style=display:flex><span>Total: 1374.5 TiB
</span></span><span style=display:flex><span>PG/OSD: 57.8458
</span></span><span style=display:flex><span>MIN/MAX VAR: 0.73/1.29  STDDEV: 6.47
</span></span></code></pre></div><p>All pools have been set to <code>ceph osd pool set &lt;pool-name> pg_autoscale_mode warn</code> and some of them have been assigned the bulk flag (<code>ceph osd pool set &lt;pool-name> bulk true</code>) to make the autoscaler expect more data in this pool
(usually your RBD pool, RGW data pool, or CephFS data pool).</p><p><strong>After:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>❯ ceph balancer eval
</span></span><span style=display:flex><span>current cluster score 0.025255 (lower is better)
</span></span><span style=display:flex><span>❯ # awk filters relevant information: min/max OSD, total, PG/OSD, and variance
</span></span><span style=display:flex><span>❯ ceph osd df |awk -v min=100 -v max=0 \
</span></span><span style=display:flex><span>    &#39;/VAR/ {print}
</span></span><span style=display:flex><span>     /hdd/ { sum+=$7; c+=1; pg+=$19; if($17&lt;min){min_line=$0;min=$17}; if($17&gt;max) {max_line=$0;max=$17} }
</span></span><span style=display:flex><span>     /TOTAL/ { print min_line; print max_line; print &#34;Total:&#34;, sum, &#34;TiB&#34;; print &#34;PG/OSD:&#34;, pg/c }&#39;
</span></span><span style=display:flex><span>ID   CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP      META     AVAIL    %USE   VAR   PGS  STATUS
</span></span><span style=display:flex><span> 56    hdd  9.27039   1.00000  9.3 TiB  4.9 TiB  4.7 TiB   1.2 MiB   10 GiB  4.4 TiB  52.47  0.86   76      up
</span></span><span style=display:flex><span>170    hdd  9.27039   1.00000  9.3 TiB  6.5 TiB  6.3 TiB   1.1 MiB   13 GiB  2.8 TiB  70.20  1.16   98      up
</span></span><span style=display:flex><span>Total: 1351.3 TiB
</span></span><span style=display:flex><span>PG/OSD: 86.6458
</span></span><span style=display:flex><span>MIN/MAX VAR: 0.86/1.16  STDDEV: 3.42
</span></span></code></pre></div><p>After increasing <code>pg_num</code> for two pools and waiting for the process to finish
(took about half a week), you can see that even though the total size did
decreased a little bit (~20TiB) the variance shown by <code>ceph balancer eval</code> and
the standard deviation shown by <code>ceph osd df</code> went down. This is also reflected
by the fullest OSD being significantly lower (79% -> 70%) and the least filled
OSD being better utilised (45% -> 52%).</p><p>Note that you should also let the balancer do its job after increasing the PG
count; so this could even improve a little bit.</p><p><img alt="Grafana panels showing distribution spread getting narrower and PG count increasing" loading=lazy src=/assets/ceph-distribution-1.png></p><p>You can see with more PGs the spread of OSD capacities narrows and focuses more
on a lower number which is exactly what you want to see. Interestingly, if you
look at the PGs per OSD you can also see that after the initial increase in the
count of PGs the distribution of PGs decreases its spread as well. This is due
to the balancer now getting the chance after the data splits to optimize their
placements.</p><p>Slight warning: Do not be alarmed if you get warning about (deep-)scrubs not
being done on time. Since increasing PG count leads to lots of backfilling
potentially over a prolonged period, Ceph prioritizes backfilling over scrubs.
After the cluster is back to a static number of PGs and finished all backfilling
task the number of scrubs not done on time should decrease (I would expect it to
take about as long as the PG increase or less).</p><p><img alt="Recovery during PG increase" loading=lazy src=/assets/ceph-recovery-1.png></p><p>To improve recovery speed I set <code>ceph config set osd osd_mclock_profile high_recovery_ops</code>. To not impact daily usage too much, I set up an ad-hoc
systemd-timers:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>systemd-run --on-calendar <span style=color:#e6db74>&#39;Mon..Fri 2024-12-* 18:00&#39;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    ceph config set osd osd_mclock_profile high_recovery_ops
</span></span><span style=display:flex><span>systemd-run --on-calendar <span style=color:#e6db74>&#39;Mon..Fri 2024-12-* 08:00&#39;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    ceph config set osd osd_mclock_profile high_client_ops
</span></span><span style=display:flex><span>systemd-run --on-calendar <span style=color:#e6db74>&#39;2025-01-01 08:00&#39;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    ceph config set osd osd_mclock_profile high_client_ops
</span></span></code></pre></div><p>Note: Keep in mind, that if the server these commands were run on restarts, the
timers are gone. So make sure to check that you have indeed set the proper
<code>osd_mclock_profile</code> after you are done. You could also remove the setting to go
back to the balance default with <code>ceph config rm osd osd_mclock_profile</code></p><h2 id=getting-your-hands-dirty>Getting Your Hands Dirty<a hidden class=anchor aria-hidden=true href=#getting-your-hands-dirty>#</a></h2><p>If this is still not enough there is more you can do, by adjusting the Ceph OSD
(re)weights.</p><p>The existing scripts only looked at the current utilization and acted upon this
information. Then you had to wait for the result and tweak another value.
I wrote a few scripts for looking at changes during backfilling and adjusting
based on future balance results.</p><p>I created a <a href=https://github.com/Syphdias/ceph-scripts>repo</a>, if you want to check them out.</p><h2 id=emergency-only>Emergency Only<a hidden class=anchor aria-hidden=true href=#emergency-only>#</a></h2><p>If you are really in a pickle and need to get a blocked cluster running again to
be able to rebalance (not to use it!), you can change what is recognized as a
backfillfull or full Ceph OSD. I do not recommend this!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>ceph osd dump | grep ratio
</span></span><span style=display:flex><span>ceph osd set-backfillfull-ratio $higher
</span></span><span style=display:flex><span>ceph osd set-full-ratio $more_higher
</span></span></code></pre></div><p><strong>Also, make sure to change it back after your incident!</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>ceph osd set-backfillfull-ratio .9
</span></span><span style=display:flex><span>ceph osd set-full-ratio .95
</span></span><span style=display:flex><span>ceph osd dump | grep ratio
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://www.relg.uk/>relg.uk</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>